import asyncio
import pandas as pd
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
import re
import random
import numpy as np
import json

BRAND_URLS = [
    "https://car.yiche.com/chaojingqiche/"

]
MAX_PAGES_PER_MODEL = 1


async def scrape_single_car_config(page, url):
    print(f"  [é…ç½®] æ­£åœ¨çˆ¬å–å‚æ•°é…ç½®é¡µ: {url}")
    try:
        await page.goto(url, timeout=60000, wait_until="networkidle")
        await page.wait_for_selector(".parameter-container", timeout=20000)
        page_html = await page.content()
    except Exception as e:
        print(f"    -> [é…ç½®] é¡µé¢åŠ è½½å¤±è´¥æˆ–æœªæ‰¾åˆ°é…ç½®è¡¨: {url}, é”™è¯¯: {e}")
        return None

    soup = BeautifulSoup(page_html, 'html.parser')
    main_model_name = "æœªçŸ¥è½¦ç³»"
    title_tag = soup.select_one('.cx-brand-info h1 em, .p-block-title, .model-name')
    if title_tag:
        main_model_name = title_tag.text.strip().replace('å‚æ•°é…ç½®', '').strip()
        print(f"    -> [é…ç½®] æˆåŠŸè¯†åˆ«è½¦ç³»ä¸º: {main_model_name}")
    else:
        print(f"    -> [é…ç½®] æœªèƒ½è‡ªåŠ¨è¯†åˆ«è½¦ç³»åç§°")

    config_container = soup.find('div', class_='parameter-container')
    table = config_container.find('table', class_='main-param-table')
    if not table: return None

    header_row = table.find('tr', class_='t-header')
    headers = [th.text.strip() for th in header_row.find_all('span', class_='car-style-info')]
    all_car_data = [{'è½¦å‹åç§°': name} for name in headers]
    current_category = "æœªçŸ¥åˆ†ç±»"
    rows = table.find('tbody').find_all('tr')
    for row in rows:
        if 't-header' in row.get('class', []): continue
        category_tag = row.find('h3')
        if category_tag:
            current_category = category_tag.text.strip()
            continue
        item_name_tag = row.find('td', attrs={'rowspan': '1'})
        if item_name_tag and item_name_tag.text.strip():
            item_name = item_name_tag.text.strip()
            full_key = f"{current_category}_{item_name}"
            values_tags = item_name_tag.find_next_siblings('td')
            values = []
            for val_tag in values_tags:
                cell_text = val_tag.get_text(strip=True)
                if 'â—' in cell_text:
                    values.append(cell_text.replace('â—', 'æ ‡é… ').strip())
                elif 'â—‹' in cell_text:
                    values.append(cell_text.replace('â—‹', 'é€‰é… ').strip())
                elif cell_text == '-':
                    values.append('æ— ')
                else:
                    values.append(cell_text if cell_text else 'æ— ')
            for i, car in enumerate(all_car_data):
                car[full_key] = values[i] if i < len(values) else 'æ— '
    for car_record in all_car_data:
        car_record['è½¦ç³»åç§°'] = main_model_name
    return pd.DataFrame(all_car_data)


async def scrape_koubei_page(page, url):
    print(f"  [å£ç¢‘] æ­£åœ¨çˆ¬å–é¡µé¢: {url}")
    try:
        await page.goto(url, timeout=60000, wait_until="domcontentloaded")
        if not await page.is_visible('.cm-content-moudle'):
            print("    -> [å£ç¢‘] é¡µé¢ä¸­æœªå‘ç°ç‚¹è¯„å†…å®¹ï¼Œåˆ¤å®šä¸ºæœ€åä¸€é¡µã€‚")
            return []
        page_html = await page.content()
    except Exception as e:
        print(f"    -> [å£ç¢‘] é¡µé¢åŠ è½½å¤±è´¥: {url}, é”™è¯¯: {e}")
        return None
    soup = BeautifulSoup(page_html, 'html.parser')
    review_modules = soup.find_all('div', class_='cm-content-moudle')
    page_reviews = []
    for module in review_modules:
        try:
            page_reviews.append({
                "è´­ä¹°è½¦å‹": module.find('p', class_='cm-car-name').text.strip() if module.find('p',
                                                                                               'cm-car-name') else "æœªçŸ¥è½¦å‹",
                "ç»¼åˆè¯„åˆ†": module.find('span', class_='score').text.strip() if module.find('span', 'score') else None,
                "ç‚¹è¯„æ ‡é¢˜": module.find('div', class_='c-info-title').text.strip() if module.find('div',
                                                                                                  'c-info-title') else "æ— æ ‡é¢˜",
                "ç‚¹è¯„å†…å®¹": module.find('div', class_='cm-content').find('p').text.strip() if module.find('div',
                                                                                                          'cm-content') and module.find(
                    'div', 'cm-content').find('p') else "",
            })
        except Exception as e:
            print(f"    -> [å£ç¢‘] è§£æå•ä¸ªç‚¹è¯„æ—¶å‡ºé”™: {e}")
            continue
    return page_reviews


async def discover_car_series(page, brand_url):
    print(f"\n--- æ­£åœ¨è®¿é—®å“ç‰Œé¡µ: {brand_url} ---")
    series_info_list = []
    try:
        await page.goto(brand_url, timeout=60000, wait_until="domcontentloaded")
        html_content = await page.content()
        soup = BeautifulSoup(html_content, 'html.parser')

        brand_name_tag = soup.select_one('.brand-name')
        brand_name = brand_name_tag.text.strip() if brand_name_tag else "æœªçŸ¥å“ç‰Œ"
        print(f"    -> [å‘ç°] æˆåŠŸè¯†åˆ«å“ç‰Œä¸º: {brand_name}")

        car_list_data = None
        try:
            car_list_data = await page.evaluate("() => window.carList")
        except Exception:
            print("    -> [å‘ç°] window.carList æœªç›´æ¥è·å–, å°è¯•ä»HTMLæºç ä¸­è§£æ...")

        if not car_list_data:
            match = re.search(r'carList\s*=\s*(\{.*?\});', html_content, re.DOTALL)
            if match:
                try:
                    json_str = match.group(1).strip()
                    car_list_data = json.loads(json_str)
                    print("    -> [å‘ç°] æˆåŠŸä»HTMLæºç ä¸­è§£æ carList JSONã€‚")
                except json.JSONDecodeError as e:
                    print(f"    -> [å‘ç°] è§£æ window.carList JSON å¤±è´¥: {e}")
                    car_list_data = None
            else:
                print("    -> [å‘ç°] æœªèƒ½åœ¨HTMLæºç ä¸­æ‰¾åˆ° carList æ•°æ®ã€‚")

        if car_list_data and "onAndWaitList" in car_list_data:
            for manufacturer in car_list_data["onAndWaitList"]:
                if "serialList" in manufacturer:
                    for car_model in manufacturer["serialList"]:
                        if car_model.get("allSpell"):
                            all_spell = car_model['allSpell']
                            image_url = car_model.get("imageUrl", "").replace('{0}', '6')
                            if image_url and not image_url.startswith('http'):
                                image_url = 'https:' + image_url

                            series_info_list.append({
                                "brand": brand_name,
                                "config_url": f"https://car.yiche.com/{all_spell}/peizhi/",
                                "koubei_url": f"https://dianping.yiche.com/{all_spell}/koubei/",
                                "å›¾ç‰‡é“¾æ¥": image_url
                            })
            print(f"å“ç‰Œ '{brand_name}' ä¸‹å‘ç°äº† {len(series_info_list)} ä¸ªè½¦ç³»ã€‚")
            return series_info_list
    except Exception as e:
        print(f"å¤„ç†å“ç‰Œé¡µ {brand_url} æ—¶å‡ºé”™: {e}")
        return []


async def main():
    all_final_data = []
    print("--- å¯åŠ¨ä¸€ä½“åŒ–çˆ¬å–ä»»åŠ¡ ---")
    async with async_playwright() as p:
        browser = await p.chromium.launch(channel="msedge", headless=True)
        page = await browser.new_page()
        for brand_url in BRAND_URLS:
            car_series_list = await discover_car_series(page, brand_url)
            for series in car_series_list:
                config_df = await scrape_single_car_config(page, series["config_url"])
                await asyncio.sleep(random.uniform(1, 2))
                if config_df is None or config_df.empty:
                    print(f"    -> è·³è¿‡è½¦ç³»ï¼Œæœªèƒ½è·å–å‚æ•°é…ç½®ã€‚ URL: {series['config_url']}")
                    continue

                series_image_url = series.get("å›¾ç‰‡é“¾æ¥", "")
                brand_name = series.get("brand", "æœªçŸ¥å“ç‰Œ")
                config_df['å›¾ç‰‡é“¾æ¥'] = series_image_url
                config_df['å“ç‰Œ'] = brand_name  # æ–°å¢å“ç‰Œåˆ—

                print(f"    -> [é…ç½®] æˆåŠŸçˆ¬å– {len(config_df)} æ¬¾å…·ä½“è½¦å‹çš„é…ç½®ã€‚")
                all_final_data.append(config_df)

                all_reviews_list = []
                base_koubei_url = series["koubei_url"]
                for page_num in range(1, MAX_PAGES_PER_MODEL + 1):
                    current_url = base_koubei_url if page_num == 1 else f"{base_koubei_url.replace('/koubei/', '')}/koubei-{page_num}.html"
                    reviews = await scrape_koubei_page(page, current_url)
                    if reviews:
                        all_reviews_list.extend(reviews)
                        print(f"    -> [å£ç¢‘] ç¬¬ {page_num} é¡µæˆåŠŸçˆ¬å– {len(reviews)} æ¡ç‚¹è¯„ã€‚")
                    else:
                        break
                    await asyncio.sleep(random.uniform(1, 3))

                if all_reviews_list:
                    reviews_df = pd.DataFrame(all_reviews_list)
                    reviews_df['ç»¼åˆè¯„åˆ†'] = pd.to_numeric(reviews_df['ç»¼åˆè¯„åˆ†'], errors='coerce')
                    reviews_df.dropna(subset=['ç»¼åˆè¯„åˆ†'], inplace=True)

                    if not reviews_df.empty:
                        reviews_df['è¯„ä»·å…¨æ–‡_å¸¦è½¦å‹'] = "[ç”¨æˆ·å¡«å†™è½¦å‹: " + reviews_df['è´­ä¹°è½¦å‹'].astype(str) + "] " + \
                                                        "ã€" + reviews_df['ç‚¹è¯„æ ‡é¢˜'].astype(str) + "ã€‘ " + \
                                                        reviews_df['ç‚¹è¯„å†…å®¹'].astype(str)

                        series_name = config_df['è½¦ç³»åç§°'].iloc[0]
                        series_avg_score = reviews_df['ç»¼åˆè¯„åˆ†'].mean()
                        series_review_count = len(reviews_df)
                        series_all_reviews = '\n\n'.join(reviews_df['è¯„ä»·å…¨æ–‡_å¸¦è½¦å‹'])

                        general_reviews_row = {
                            'å“ç‰Œ': brand_name,
                            'è½¦ç³»åç§°': series_name,
                            'è½¦å‹åç§°': f"[{series_name} è½¦ç³»é€šç”¨å£ç¢‘]",
                            'å¹³å‡è¯„åˆ†': round(series_avg_score, 2),
                            'è¯„ä»·æ•°é‡': series_review_count,
                            'æ‰€æœ‰è¯„ä»·': series_all_reviews,
                            'å›¾ç‰‡é“¾æ¥': series_image_url
                        }
                        all_final_data.append(pd.DataFrame([general_reviews_row]))
                        print(
                            f"    -> [æ•´åˆ] å·²ä¸ºè½¦ç³» '{series_name}' åˆ›å»ºäº†åŒ…å« {series_review_count} æ¡å£ç¢‘çš„ç‹¬ç«‹æ±‡æ€»è¡Œã€‚")
        await browser.close()

    if not all_final_data:
        print("\n--- ä»»åŠ¡å®Œæˆï¼Œä½†æ²¡æœ‰çˆ¬å–åˆ°ä»»ä½•æ•°æ®ã€‚ ---")
        return

    print("\n--- æ‰€æœ‰æ•°æ®çˆ¬å–å®Œæ¯•ï¼Œå¼€å§‹è¿›è¡Œæœ€ç»ˆæ•´åˆä¸ä¿å­˜ ---")
    master_df = pd.concat(all_final_data, ignore_index=True)
    if 'è¯„ä»·æ•°é‡' not in master_df.columns:
        master_df['è¯„ä»·æ•°é‡'] = 0
    else:
        master_df['è¯„ä»·æ•°é‡'] = master_df['è¯„ä»·æ•°é‡'].fillna(0).astype(int)

    if 'æ‰€æœ‰è¯„ä»·' not in master_df.columns:
        master_df['æ‰€æœ‰è¯„ä»·'] = ''
    else:
        master_df['æ‰€æœ‰è¯„ä»·'] = master_df['æ‰€æœ‰è¯„ä»·'].fillna('')

    if 'å¹³å‡è¯„åˆ†' not in master_df.columns:
        master_df['å¹³å‡è¯„åˆ†'] = np.nan
    if 'å›¾ç‰‡é“¾æ¥' not in master_df.columns:
        master_df['å›¾ç‰‡é“¾æ¥'] = ''
    master_df['å›¾ç‰‡é“¾æ¥'] = master_df['å›¾ç‰‡é“¾æ¥'].fillna('')

    if 'å“ç‰Œ' not in master_df.columns:
        master_df['å“ç‰Œ'] = 'æœªçŸ¥å“ç‰Œ'
    master_df['å“ç‰Œ'] = master_df['å“ç‰Œ'].fillna('æœªçŸ¥å“ç‰Œ')

    core_cols = ['å“ç‰Œ', 'è½¦ç³»åç§°', 'è½¦å‹åç§°', 'å¹³å‡è¯„åˆ†', 'è¯„ä»·æ•°é‡', 'æ‰€æœ‰è¯„ä»·', 'å›¾ç‰‡é“¾æ¥']
    config_cols = [col for col in master_df.columns if col not in core_cols]
    new_order = core_cols + sorted(config_cols)
    master_df = master_df.reindex(columns=new_order)

    output_filename = f"æ˜“è½¦_è¶…å¢ƒå‚æ•°ä¸å£ç¢‘æ±‡æ€».csv"
    master_df.to_csv(output_filename, index=False, encoding='utf-8-sig')

    print(f"\nğŸ‰ğŸ‰ğŸ‰ ä¸€ä½“åŒ–çˆ¬å–ä¸æ•´åˆä»»åŠ¡å…¨éƒ¨å®Œæˆï¼ğŸ‰ğŸ‰ğŸ‰")
    print(f"å…±ç”Ÿæˆ {len(master_df)} æ¡æ•°æ®è®°å½•ï¼ˆåŒ…å«è½¦å‹é…ç½®ä¸å£ç¢‘æ±‡æ€»è¡Œï¼‰ã€‚")
    print(f"æ•°æ®å·²å…¨éƒ¨æ±‡æ€»å¹¶ä¿å­˜è‡³: {output_filename}")


if __name__ == "__main__":
    asyncio.run(main())
