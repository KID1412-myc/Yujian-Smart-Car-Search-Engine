import os
from dotenv import load_dotenv
load_dotenv()
import traceback
from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
from elasticsearch import Elasticsearch, NotFoundError
from openai import OpenAI
import re
import json
import sys
import subprocess
import time
from werkzeug.utils import secure_filename

MAPPING_FILE_PATH = os.path.join(os.path.dirname(__file__), 'feature_mapping.json')
from flask_sqlalchemy import SQLAlchemy
from flask_jwt_extended import (
    JWTManager, create_access_token, get_jwt_identity,
    jwt_required, set_access_cookies, unset_jwt_cookies,
    JWTManager, get_jwt
)
import bcrypt
from datetime import timedelta
from flask_migrate import Migrate
from functools import wraps

# æ ¸å¿ƒé…ç½® (AI, ES, Flask)
DEEPSEEK_API_KEY = os.environ.get("DEEPSEEK_API_KEY")
DEEPSEEK_MODEL_NAME = "deepseek-chat"
PROXY_URL = ""
ES_HOST = "localhost"
ES_PORT = 9200
INDEX_NAME = "yiche_cars"
UPLOAD_FOLDER = os.path.join(os.path.dirname(__file__), 'temp_uploads')
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
CRAWLER_SCRIPT_PATH = os.path.join(os.path.dirname(__file__), 'web_crawler_dynamic.py')
IMPORTER_SCRIPT_PATH = os.path.join(os.path.dirname(__file__), 'web_batch_importer.py')
CRAWLER_OUTPUT_FILE = "latest_crawl_output.csv"
CRAWLER_STATUS_FILE = os.path.join(os.path.dirname(__file__), "crawler.status")
CRAWLER_LOG_FILE = os.path.join(os.path.dirname(__file__), "crawler.log")
app = Flask(__name__)
CORS(app, supports_credentials=True)

app.config['SQLALCHEMY_DATABASE_URI'] = os.environ.get("DATABASE_URL")
app.config['JWT_SECRET_KEY'] = os.environ.get("JWT_SECRET_KEY")
app.config["JWT_COOKIE_CSRF_PROTECT"] = False
app.config["JWT_COOKIE_SECURE"] = False
app.config["JWT_COOKIE_SAMESITE_POLICY"] = "None"
app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False
app.config["JWT_TOKEN_LOCATION"] = ["headers", "cookies"]
app.config["JWT_ACCESS_TOKEN_EXPIRES"] = timedelta(days=30)

db = SQLAlchemy(app)
jwt = JWTManager(app)
migrate = Migrate(app, db)

# AI æœåŠ¡é…ç½®
try:
    if PROXY_URL:
        os.environ['http_proxy'] = PROXY_URL
        os.environ['https_proxy'] = PROXY_URL
    llm_client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url="https://api.deepseek.com")
    print("AI æœåŠ¡é…ç½®æˆåŠŸï¼")
except Exception as e:
    print(f"AI æœåŠ¡é…ç½®å¤±è´¥: {e}")
    llm_client = None

# ES é…ç½®
try:
    es_client = Elasticsearch([{'host': ES_HOST, 'port': ES_PORT, 'scheme': 'http'}])
    if not es_client.ping():
        raise ConnectionError("æ— æ³•è¿æ¥åˆ°Elasticsearch")
    print(f"åç«¯æˆåŠŸè¿æ¥åˆ°Elasticsearchï¼ç›®æ ‡ç´¢å¼•: {INDEX_NAME}")
except Exception as e:
    print(f"è¿æ¥Elasticsearchå¤±è´¥: {e}")
    es_client = None

# åŠ è½½ç‰¹æ€§æ˜ å°„æ–‡ä»¶
FEATURE_MAPPING = {}
MAPPING_FILE_PATH = os.path.join(os.path.dirname(__file__), 'feature_mapping.json')
try:
    with open(MAPPING_FILE_PATH, 'r', encoding='utf-8') as f:
        FEATURE_MAPPING = json.load(f)
    print(f"ç‰¹æ€§æ˜ å°„æ–‡ä»¶ {MAPPING_FILE_PATH} åŠ è½½æˆåŠŸï¼å…± {len(FEATURE_MAPPING)} ä¸ªç‰¹æ€§ã€‚")
except FileNotFoundError:
    print(f"è­¦å‘Šï¼šç‰¹æ€§æ˜ å°„æ–‡ä»¶ feature_mapping.json æœªæ‰¾åˆ°ï¼Œç‰¹æ€§æœç´¢å°†å›é€€åˆ°AIè§£æã€‚")
except json.JSONDecodeError as e:
    print(f"é”™è¯¯ï¼šè§£æç‰¹æ€§æ˜ å°„æ–‡ä»¶ feature_mapping.json å¤±è´¥: {e}")
except Exception as e:
    print(f"åŠ è½½ç‰¹æ€§æ˜ å°„æ–‡ä»¶æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")


# æ•°æ®åº“æ¨¡å‹ (Pythonç±» æ˜ å°„ MySQLè¡¨)

class User(db.Model):
    __tablename__ = 'users'
    id = db.Column(db.Integer, primary_key=True)
    username = db.Column(db.String(255), unique=True, nullable=False)
    hashed_password = db.Column(db.String(255), nullable=False)
    nickname = db.Column(db.String(255), nullable=True)
    role = db.Column(db.String(50), nullable=False, default='user')
    is_banned = db.Column(db.Boolean, nullable=False, default=False)
    ban_reason = db.Column(db.Text, nullable=True)
    favorites = db.relationship('Favorite', backref='user', lazy=True, cascade="all, delete-orphan")

    def __init__(self, username, password, nickname=None):
        self.username = username
        self.hashed_password = bcrypt.hashpw(password.encode('utf-8'), bcrypt.gensalt()).decode('utf-8')
        if nickname:
            self.nickname = nickname
        else:
            self.nickname = username

    def check_password(self, password):
        return bcrypt.checkpw(password.encode('utf-8'), self.hashed_password.encode('utf-8'))


class Favorite(db.Model):
    __tablename__ = 'favorites'
    id = db.Column(db.Integer, primary_key=True)
    user_id = db.Column(db.Integer, db.ForeignKey('users.id'), nullable=False)
    model_id = db.Column(db.String(255), nullable=False)
    series_name = db.Column(db.String(255), nullable=False)
    __table_args__ = (db.UniqueConstraint('user_id', 'model_id', name='_user_model_uc'),)


class SystemConfig(db.Model):
    __tablename__ = 'system_config'
    key = db.Column(db.String(50), primary_key=True)
    value = db.Column(db.String(255), nullable=True)

    def __init__(self, key, value):
        self.key = key
        self.value = value


# è¾…åŠ©å‡½æ•°ï¼šéªŒè¯é«˜å±æ“ä½œå¯†ç 
def _verify_high_risk_password(password):
    if not password:
        return False, "å¿…é¡»æä¾›é«˜å±æ“ä½œå¯†ç "
    config = SystemConfig.query.get('high_risk_password')

    if not config or not config.value:
        return False, "é«˜å±æ“ä½œå¯†ç å°šæœªè®¾ç½®ï¼Œæ“ä½œè¢«é˜»æ­¢ã€‚è¯·æ ¸å¿ƒç®¡ç†å‘˜å…ˆè®¾ç½®å¯†ç ã€‚"
    if bcrypt.checkpw(password.encode('utf-8'), config.value.encode('utf-8')):
        return True, "éªŒè¯é€šè¿‡"
    else:
        return False, "é«˜å±æ“ä½œå¯†ç ä¸æ­£ç¡®"


def parse_price_to_numeric(price_str):
    if not isinstance(price_str, str) or 'ä¸‡' not in price_str:
        return None
    match = re.search(r'(\d+\.?\d*)', price_str)
    if match:
        try:
            return float(match.group(1))
        except (ValueError, IndexError):
            return None
    return None


def clean_power_type(energy_type_str):
    if not isinstance(energy_type_str, str): return 'æœªçŸ¥'
    if 'æ’ç”µæ··' in energy_type_str: return 'æ’ç”µæ··åŠ¨'
    if 'å¢ç¨‹' in energy_type_str: return 'å¢ç¨‹å¼'
    if 'æ²¹ç”µæ··' in energy_type_str: return 'æ²¹ç”µæ··åˆ'
    if 'è½»æ··' in energy_type_str: return 'è½»æ··ç³»ç»Ÿ'
    if 'æ°¢' in energy_type_str: return 'æ°¢èƒ½æº'
    if 'æ±½æ²¹' in energy_type_str: return 'ç‡ƒæ²¹'
    if 'çº¯ç”µ' in energy_type_str: return 'çº¯ç”µ'
    return energy_type_str if energy_type_str else 'æœªçŸ¥'


def clean_body_type(level_str):
    if not isinstance(level_str, str): return 'å…¶ä»–'
    if 'SUV' in level_str.upper(): return 'SUV'
    if 'MPV' in level_str.upper(): return 'MPV'
    if 'è·‘è½¦' in level_str: return 'è·‘è½¦'
    if 'æ—…è¡Œè½¦' in level_str: return 'æ—…è¡Œè½¦'
    if 'æ€èƒŒè½¦' in level_str: return 'æ€èƒŒè½¦'
    if 'æ•ç¯·è½¦' in level_str: return 'æ•ç¯·è½¦'
    if 'çš®å¡' in level_str: return 'çš®å¡'
    if 'ä¸¤å¢è½¦' in level_str or 'ä¸‰å¢è½¦' in level_str: return 'è½¿è½¦'
    return 'å…¶ä»–'


def clean_seat_count(structure_str):
    if not isinstance(structure_str, str): return None
    if '2åº§' in structure_str: return '2åº§'
    if '4åº§' in structure_str: return '4åº§'
    if '5åº§' in structure_str: return '5åº§'
    if '6åº§' in structure_str: return '6åº§'
    if '7åº§' in structure_str: return '7åº§'
    return None


def clean_segment(level_str):
    if not isinstance(level_str, str): return None
    if 'å°å‹' in level_str: return 'å°å‹'
    if 'ç´§å‡‘å‹' in level_str: return 'ç´§å‡‘å‹'
    if 'ä¸­å‹' in level_str: return 'ä¸­å‹'
    if 'ä¸­å¤§å‹' in level_str: return 'ä¸­å¤§å‹'
    if 'å¤§å‹' in level_str: return 'å¤§å‹'
    return None


def get_current_user_from_jwt():
    try:
        current_user_id = get_jwt_identity()
        user = User.query.get(int(current_user_id))
        return user
    except Exception:
        return None


def core_admin_required():
    def wrapper(fn):
        @wraps(fn)
        @jwt_required()
        def decorator(*args, **kwargs):
            user = get_current_user_from_jwt()
            if user and user.role == 'core_admin':
                return fn(*args, **kwargs)
            else:
                return jsonify({"error": "æƒé™ä¸è¶³ï¼šéœ€è¦æ ¸å¿ƒç®¡ç†å‘˜æƒé™"}), 403

        return decorator

    return wrapper


def admin_required():
    def wrapper(fn):
        @wraps(fn)
        @jwt_required()
        def decorator(*args, **kwargs):
            user = get_current_user_from_jwt()
            if user and (user.role == 'admin' or user.role == 'core_admin'):
                return fn(*args, **kwargs)
            else:
                return jsonify({"error": "æƒé™ä¸è¶³ï¼šéœ€è¦ç®¡ç†å‘˜æƒé™"}), 403

        return decorator

    return wrapper


@app.route('/auth/register', methods=['POST'])
def register():
    try:
        data = request.json
        username = data.get('username')
        password = data.get('password')

        if not username or not password:
            return jsonify({"error": "å¿…é¡»æä¾›ç”¨æˆ·åå’Œå¯†ç "}), 400
        existing_user = User.query.filter_by(username=username).first()
        if existing_user:
            return jsonify({"error": "è¯¥ç”¨æˆ·åå·²è¢«å ç”¨"}), 409
        new_user = User(username=username, password=password)
        db.session.add(new_user)
        db.session.commit()

        return jsonify({"message": "æ³¨å†ŒæˆåŠŸï¼"}), 201

    except Exception as e:
        db.session.rollback()
        traceback.print_exc()
        return jsonify({"error": f"æ³¨å†Œæ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {str(e)}"}), 500


@app.route('/auth/login', methods=['POST'])
def login():
    try:
        data = request.json
        username = data.get('username')
        password = data.get('password')

        if not username or not password:
            return jsonify({"error": "å¿…é¡»æä¾›ç”¨æˆ·åå’Œå¯†ç "}), 400

        user = User.query.filter_by(username=username).first()

        if not user or not user.check_password(password):
            return jsonify({"error": "ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯"}), 401

        if user.is_banned:
            reason = user.ban_reason if user.ban_reason else "æ— ç‰¹å®šåŸå› "
            return jsonify({
                "error": "æ­¤è´¦æˆ·å·²è¢«å°ç¦",
                "reason": reason
            }), 403

        access_token = create_access_token(identity=str(user.id))

        response_data = {
            "message": "ç™»å½•æˆåŠŸ",
            "user": {
                "username": user.username,
                "nickname": user.nickname,
                "role": user.role
            }
        }

        response = jsonify(response_data)
        set_access_cookies(response, access_token)

        print(f"   -> (ç™»å½•æˆåŠŸ: {username}, è§’è‰²: {user.role})")
        return response, 200

    except Exception as e:
        traceback.print_exc()
        return jsonify({"error": f"ç™»å½•æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {str(e)}"}), 500


@app.route('/auth/logout', methods=['POST'])
def logout():
    response = jsonify({"message": "é€€å‡ºç™»å½•æˆåŠŸ"})
    unset_jwt_cookies(response)
    return response, 200


@app.route('/api/profile', methods=['GET'])
@jwt_required()
def get_profile():
    try:
        current_user_id = get_jwt_identity()
        user = User.query.filter_by(id=int(current_user_id)).first()

        if not user:
            return jsonify({"error": "ç”¨æˆ·ä¸å­˜åœ¨"}), 404

        return jsonify({
            "id": user.id,
            "username": user.username,
            "nickname": user.nickname,
            "role": user.role
        }), 200

    except Exception as e:
        traceback.print_exc()
        return jsonify({"error": f"è·å–ç”¨æˆ·ä¿¡æ¯æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {str(e)}"}), 500


@app.route('/api/profile/update', methods=['POST'])
@jwt_required()
def update_profile():
    try:
        current_user_id = get_jwt_identity()
        user = User.query.filter_by(id=int(current_user_id)).first()

        if not user:
            return jsonify({"error": "ç”¨æˆ·ä¸å­˜åœ¨"}), 404

        data = request.json
        new_nickname = data.get('nickname')

        if not new_nickname or len(new_nickname.strip()) == 0:
            return jsonify({"error": "æ˜µç§°ä¸èƒ½ä¸ºç©º"}), 400

        user.nickname = new_nickname.strip()
        db.session.commit()

        print(f"   -> (ç”¨æˆ· {user.username} æ˜µç§°æ›´æ–°ä¸º: {new_nickname})")
        return jsonify({"message": "æ˜µç§°æ›´æ–°æˆåŠŸ", "nickname": user.nickname}), 200

    except Exception as e:
        db.session.rollback()
        traceback.print_exc()
        return jsonify({"error": f"æ›´æ–°æ˜µç§°æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {str(e)}"}), 500


@app.route('/api/profile/change_password', methods=['POST'])
@jwt_required()
def change_password():
    try:
        current_user_id = get_jwt_identity()
        user = User.query.get(int(current_user_id))
        if not user:
            return jsonify({"error": "ç”¨æˆ·ä¸å­˜åœ¨"}), 404

        data = request.json
        current_password = data.get('current_password')
        new_password = data.get('new_password')

        if not current_password or not new_password:
            return jsonify({"error": "ç¼ºå°‘å¿…è¦å‚æ•°"}), 400

        if not user.check_password(current_password):
            return jsonify({"error": "å½“å‰å¯†ç ä¸æ­£ç¡®"}), 401
        user.hashed_password = bcrypt.hashpw(new_password.encode('utf-8'), bcrypt.gensalt()).decode('utf-8')
        db.session.commit()

        print(f"   -> (ç”¨æˆ· {user.username} å¯†ç å·²æ›´æ–°)")
        return jsonify({"message": "å¯†ç æ›´æ–°æˆåŠŸ"}), 200

    except Exception as e:
        db.session.rollback()
        traceback.print_exc()
        return jsonify({"error": f"æ›´æ–°å¯†ç æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {str(e)}"}), 500


@app.route('/api/favorites', methods=['GET'])
@jwt_required()
def get_favorites():
    try:
        current_user_id = get_jwt_identity()
        user = User.query.filter_by(id=int(current_user_id)).first()

        if not user:
            response = jsonify({"error": "ç”¨æˆ·ä¸å­˜åœ¨"})
            unset_jwt_cookies(response)
            return response, 404

        favorites = user.favorites
        fav_list = [{"id": f.model_id, "series": f.series_name} for f in favorites]

        return jsonify(fav_list), 200

    except Exception as e:
        traceback.print_exc()
        return jsonify({"error": f"è·å–æ”¶è—æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {str(e)}"}), 500


@app.route('/api/favorites/add', methods=['POST'])
@jwt_required()
def add_favorite():
    try:
        current_user_id = get_jwt_identity()
        user = User.query.filter_by(id=int(current_user_id)).first()

        if not user:
            return jsonify({"error": "ç”¨æˆ·ä¸å­˜åœ¨"}), 404

        data = request.json
        model_id = data.get('model_id')
        series_name = data.get('series_name')

        if not model_id or not series_name:
            return jsonify({"error": "ç¼ºå°‘ model_id æˆ– series_name"}), 400

        existing_fav = Favorite.query.filter_by(user_id=user.id, model_id=model_id).first()
        if existing_fav:
            return jsonify({"message": "å·²æ”¶è—"}), 200

        new_fav = Favorite(user_id=user.id, model_id=model_id, series_name=series_name)
        db.session.add(new_fav)
        db.session.commit()

        print(f"   -> (ç”¨æˆ·ID {user.id} æ·»åŠ æ”¶è—: {model_id})")
        return jsonify({"message": "æ·»åŠ æ”¶è—æˆåŠŸ"}), 201

    except Exception as e:
        db.session.rollback()
        traceback.print_exc()
        return jsonify({"error": f"æ·»åŠ æ”¶è—æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {str(e)}"}), 500


@app.route('/api/favorites/remove', methods=['POST'])
@jwt_required()
def remove_favorite():
    try:
        current_user_id = get_jwt_identity()
        user = User.query.filter_by(id=int(current_user_id)).first()

        if not user:
            return jsonify({"error": "ç”¨æˆ·ä¸å­˜åœ¨"}), 404

        data = request.json
        model_id = data.get('model_id')

        if not model_id:
            return jsonify({"error": "ç¼ºå°‘ model_id"}), 400

        fav_to_remove = Favorite.query.filter_by(user_id=user.id, model_id=model_id).first()

        if fav_to_remove:
            db.session.delete(fav_to_remove)
            db.session.commit()
            print(f"   -> (ç”¨æˆ·ID {user.id} ç§»é™¤æ”¶è—: {model_id})")
            return jsonify({"message": "ç§»é™¤æ”¶è—æˆåŠŸ"}), 200
        else:
            return jsonify({"message": "æ”¶è—ä¸å­˜åœ¨"}), 404

    except Exception as e:
        db.session.rollback()
        traceback.print_exc()
        return jsonify({"error": f"ç§»é™¤æ”¶è—æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {str(e)}"}), 500


@app.route('/search', methods=['POST'])
def search():
    if not es_client:
        return jsonify({"error": "æ•°æ®åº“æœåŠ¡æœªè¿æ¥"}), 503
    try:
        data = request.get_json()
        page = data.get('page', 1)
        per_page = data.get('per_page', 12)

        sort_by = data.get('sort_by', 'relevance')

        filter_conditions = [
            {"term": {"is_koubei_row": False}},
            {"exists": {"field": "è½¦ç³»åç§°.keyword"}}
        ]
        brand = data.get('brand')
        if brand and brand != 'æ‰€æœ‰å“ç‰Œ':
            filter_conditions.append({"term": {"å“ç‰Œ.keyword": brand}})
        manufacturer = data.get('manufacturer')
        if manufacturer and manufacturer != 'æ‰€æœ‰å‚å•†':
            if isinstance(manufacturer, list):
                filter_conditions.append({"terms": {"åŸºæœ¬ä¿¡æ¯_å‚å•†.keyword": manufacturer}})
            else:
                filter_conditions.append({"term": {"åŸºæœ¬ä¿¡æ¯_å‚å•†.keyword": manufacturer}})
        price_min = data.get('price_min')
        price_max = data.get('price_max')
        if price_min is not None and price_max is not None and (price_min > 0 or price_max > 0):
            price_filter = {}
            if price_min > 0: price_filter["gte"] = price_min
            if price_max > 0: price_filter["lte"] = price_max
            filter_conditions.append({"range": {"price_numeric": price_filter}})
        power_type = data.get('power_type')
        if power_type and power_type != 'ä¸é™':
            filter_conditions.append({"term": {"åŠ¨åŠ›ç±»å‹": power_type}})
        body_type = data.get('body_type')
        if body_type and body_type != 'ä¸é™':
            filter_conditions.append({"term": {"è½¦èº«ç±»å‹": body_type}})
        seat_count = data.get('seat_count')
        if seat_count and seat_count != 'ä¸é™':
            filter_conditions.append({"term": {"è½¦èº«_åº§ä½æ•°": seat_count}})
        segment = data.get('segment')
        if segment and segment != 'ä¸é™':
            filter_conditions.append({"term": {"åŸºæœ¬ä¿¡æ¯_çº§åˆ«": segment}})
        search_query = data.get('q', '').strip()

        base_query = {
            "bool": {
                "must": filter_conditions,
                "should": [],
                "minimum_should_match": 0
            }
        }

        if search_query:
            base_query["bool"]["minimum_should_match"] = 1

            base_query["bool"]["should"].append({
                "match": {
                    "è½¦ç³»åç§°": {
                        "query": search_query,
                        "boost": 20.0,
                        "analyzer": "my_custom_search_analyzer"
                    }
                }
            })
            base_query["bool"]["should"].append({
                "match_phrase_prefix": {
                    "è½¦ç³»åç§°": {
                        "query": search_query,
                        "boost": 15.0,
                        "analyzer": "my_custom_search_analyzer"
                    }
                }
            })
            base_query["bool"]["should"].append({
                "multi_match": {
                    "query": search_query,
                    "fields": ["è½¦å‹åç§°", "å“ç‰Œ", "åŸºæœ¬ä¿¡æ¯_å‚å•†"],
                    "type": "phrase_prefix",
                    "boost": 10.0,
                    "analyzer": "my_custom_search_analyzer"
                }
            })

            base_query["bool"]["should"].append({
                "wildcard": {
                    "è½¦ç³»åç§°.keyword": {
                        "value": f"*{search_query}*",
                        "boost": 9.0,
                        "case_insensitive": True
                    }
                }
            })

            base_query["bool"]["should"].append({
                "wildcard": {
                    "è½¦å‹åç§°.keyword": {
                        "value": f"*{search_query}*",
                        "boost": 8.0,
                        "case_insensitive": True
                    }
                }
            })
            base_query["bool"]["should"].append({
                "multi_match": {
                    "query": search_query,
                    "fields": ["è½¦å‹åç§°", "è½¦ç³»åç§°", "å“ç‰Œ"],
                    "type": "most_fields",
                    "fuzziness": "AUTO",
                    "boost": 1.0
                }
            })
            final_query = base_query
        else:
            final_query = base_query

        sort_order_config = {
            "max_relevance_score": "desc"
        }
        if sort_by == 'price_asc':
            sort_order_config = {"min_price": "asc"}
        elif sort_by == 'price_desc':
            sort_order_config = {"max_price": "desc"}

        es_query = {
            "query": final_query,
            "size": 0,
            "aggs": {
                "unique_series": {
                    "terms": {
                        "field": "è½¦ç³»åç§°.keyword",
                        "size": 1000,
                        "order": sort_order_config
                    },
                    "aggs": {
                        "max_relevance_score": {"max": {"script": "_score"}},
                        "a_representative_doc": {
                            "top_hits": {
                                "size": 1,
                                "_source": ["è½¦ç³»åç§°", "å›¾ç‰‡é“¾æ¥", "åŸºæœ¬ä¿¡æ¯_å‚å•†"]
                            }
                        },
                        "min_price": {"min": {"field": "price_numeric", "missing": 99999}},
                        "max_price": {"max": {"field": "price_numeric", "missing": 0}},
                        "power_types": {"terms": {"field": "åŠ¨åŠ›ç±»å‹"}},
                        "body_types": {"terms": {"field": "è½¦èº«ç±»å‹"}},
                        "seat_counts": {"terms": {"field": "è½¦èº«_åº§ä½æ•°"}},
                        "segments": {"terms": {"field": "åŸºæœ¬ä¿¡æ¯_çº§åˆ«"}}
                    }
                }
            }
        }

        response = es_client.search(index=INDEX_NAME, body=es_query)
        buckets = response['aggregations']['unique_series']['buckets']
        total_series = len(buckets)
        paginated_buckets = buckets[(page - 1) * per_page: page * per_page]
        results = []
        for bucket in paginated_buckets:
            hit_source = bucket['a_representative_doc']['hits']['hits'][0]['_source']
            min_p = bucket['min_price']['value']
            max_p = bucket['max_price']['value']

            if min_p == 99999: min_p = None
            if max_p == 0: max_p = None

            price_range = "æš‚æ— ä»·æ ¼"
            if min_p is not None and max_p is not None:
                price_range = f"{min_p:.2f}ä¸‡" if min_p == max_p else f"{min_p:.2f}-{max_p:.2f}ä¸‡"
            elif min_p is not None:
                price_range = f"{min_p:.2f}ä¸‡èµ·"
            elif max_p is not None:
                price_range = f"{max_p:.2f}ä¸‡"

            power_types_list = [item['key'] for item in bucket['power_types']['buckets']]
            body_types_list = [item['key'] for item in bucket['body_types']['buckets']]
            seat_counts_list = [item['key'] for item in bucket['seat_counts']['buckets']]
            segments_list = [item['key'] for item in bucket['segments']['buckets']]
            results.append({
                "_source": {
                    "å‚å•†": hit_source.get("åŸºæœ¬ä¿¡æ¯_å‚å•†"),
                    "è½¦ç³»åç§°": hit_source.get("è½¦ç³»åç§°"),
                    "å›¾ç‰‡é“¾æ¥": hit_source.get("å›¾ç‰‡é“¾æ¥", "default_image_url.jpg"),
                    "ä»·æ ¼èŒƒå›´": price_range,
                    "åŠ¨åŠ›ç±»å‹åˆ—è¡¨": power_types_list,
                    "è½¦èº«ç±»å‹åˆ—è¡¨": body_types_list,
                    "åº§ä½æ•°åˆ—è¡¨": seat_counts_list,
                    "çº§åˆ«åˆ—è¡¨": segments_list
                }
            })
        return jsonify({'total': total_series, 'hits': results})
    except Exception as e:
        traceback.print_exc()
        return jsonify({'error': str(e), 'total': 0, 'hits': []}), 500


@app.route('/get-brands-and-manufacturers', methods=['GET'])
def get_brands_and_manufacturers():
    if not es_client:
        return jsonify({"error": "æ•°æ®åº“æœåŠ¡æœªè¿æ¥"}), 503
    try:
        es_query = {
            "query": {
                "bool": {"must": [{"term": {"is_koubei_row": False}}, {"exists": {"field": "è½¦ç³»åç§°.keyword"}}]}
            },
            "size": 0,
            "aggs": {
                "brands": {
                    "terms": {"field": "å“ç‰Œ.keyword", "size": 500},
                    "aggs": {
                        "manufacturers": {"terms": {"field": "åŸºæœ¬ä¿¡æ¯_å‚å•†.keyword", "size": 100}}
                    }
                }
            }
        }
        response = es_client.search(index=INDEX_NAME, body=es_query)
        brand_to_manufacturers = {}
        for brand_bucket in response['aggregations']['brands']['buckets']:
            brand_name = brand_bucket['key']
            manufacturers_list = [manu_bucket['key'] for manu_bucket in brand_bucket['manufacturers']['buckets']]
            if brand_name and manufacturers_list:
                brand_to_manufacturers[brand_name] = manufacturers_list
        return jsonify(brand_to_manufacturers)
    except Exception as e:
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500


@app.route('/details', methods=['GET'])
def get_details():
    if not es_client:
        return jsonify({"error": "æ•°æ®åº“æœåŠ¡æœªè¿æ¥"}), 503
    try:
        series_name = request.args.get('series_name', type=str)
        if not series_name:
            return jsonify({"error": "å¿…é¡»æä¾›è½¦ç³»åç§°"}), 400
        page = request.args.get('page', 1, type=int)
        per_page = request.args.get('per_page', 50, type=int)

        query_conditions_must = [
            {"term": {"è½¦ç³»åç§°.keyword": series_name}},
            {"term": {"is_koubei_row": False}}
        ]
        query_conditions_should = []

        price_min = request.args.get('price_min', 0, type=float)
        price_max = request.args.get('price_max', 0, type=float)
        if price_min > 0 or price_max > 0:
            price_filter = {}
            if price_min > 0: price_filter["gte"] = price_min
            if price_max > 0: price_filter["lte"] = price_max
            query_conditions_must.append({"range": {"price_numeric": price_filter}})
        power_type = request.args.get('power_type', type=str)
        if power_type and power_type != 'ä¸é™':
            query_conditions_must.append({"term": {"åŠ¨åŠ›ç±»å‹": power_type}})
        body_type = request.args.get('body_type', type=str)
        if body_type and body_type != 'ä¸é™':
            query_conditions_must.append({"term": {"è½¦èº«ç±»å‹": body_type}})
        seat_count = request.args.get('seat_count', type=str)
        if seat_count and seat_count != 'ä¸é™':
            query_conditions_must.append({"term": {"è½¦èº«_åº§ä½æ•°": seat_count}})
        segment = request.args.get('segment', type=str)
        if segment and segment != 'ä¸é™':
            query_conditions_must.append({"term": {"åŸºæœ¬ä¿¡æ¯_çº§åˆ«": segment}})

        search_query = request.args.get('q', '').strip()

        if search_query:
            print(f"   -> Details Page Search: '{search_query}' - Sorting by relevance.")

            query_conditions_should.append({
                "multi_match": {"query": search_query, "fields": ["è½¦å‹åç§°"], "type": "bool_prefix", "boost": 10.0,
                                "analyzer": "my_custom_search_analyzer"}
            })
            query_conditions_should.append({
                "wildcard": {"è½¦å‹åç§°.keyword": {"value": f"*{search_query}*", "boost": 8.0, "case_insensitive": True}}
            })
            query_conditions_should.append({
                "multi_match": {"query": search_query, "fields": ["è½¦å‹åç§°"], "type": "most_fields",
                                "fuzziness": "AUTO", "boost": 1.0}
            })

            es_query = {
                "from": (page - 1) * per_page, "size": per_page,
                "query": {"bool": {"must": query_conditions_must, "should": query_conditions_should,
                                   "minimum_should_match": 1}},
                "sort": [{"_score": {"order": "desc"}}, {"price_numeric": {"order": "asc", "missing": "_last"}}]
            }
        else:
            print("   -> Details Page Browse - Sorting by price.")
            es_query = {
                "from": (page - 1) * per_page, "size": per_page,
                "query": {"bool": {"must": query_conditions_must}},
                "sort": [{"price_numeric": {"order": "asc", "missing": "_last"}}]
            }

        response = es_client.search(index=INDEX_NAME, body=es_query)
        hits = [hit['_source'] for hit in response['hits']['hits']]
        total_hits = response['hits']['total']['value']
        return jsonify({'hits': hits, 'total': total_hits, 'page': page, 'per_page': per_page})
    except Exception as e:
        traceback.print_exc()
        return jsonify({"error": f"è·å–è½¦å‹è¯¦æƒ…æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {e}"}), 500


@app.route('/get-series-by-manufacturer', methods=['GET'])
def get_series_by_manufacturer():
    if not es_client:
        return jsonify({"error": "æ•°æ®åº“æœåŠ¡æœªè¿æ¥"}), 503
    try:
        brand = request.args.get('brand', type=str)
        manufacturer = request.args.get('manufacturer', type=str)
        if not brand:
            return jsonify({"error": "å¿…é¡»æä¾›å“ç‰Œåç§°"}), 400
        query_conditions = [
            {"term": {"is_koubei_row": False}},
            {"term": {"å“ç‰Œ.keyword": brand}}
        ]
        if manufacturer and manufacturer != "æ‰€æœ‰å‚å•†":
            query_conditions.append({"term": {"åŸºæœ¬ä¿¡æ¯_å‚å•†.keyword": manufacturer}})
        es_query = {
            "query": {"bool": {"must": query_conditions}},
            "size": 0,
            "aggs": {"series_names": {"terms": {"field": "è½¦ç³»åç§°.keyword", "size": 500}}}
        }
        response = es_client.search(index=INDEX_NAME, body=es_query)
        series_list = [bucket['key'] for bucket in response['aggregations']['series_names']['buckets']]
        return jsonify(series_list)
    except Exception as e:
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500


@app.route('/search-models', methods=['GET'])
def search_models():
    if not es_client:
        return jsonify({"error": "æ•°æ®åº“æœåŠ¡æœªè¿æ¥"}), 503
    try:
        q = request.args.get('q', '').strip()
        if not q:
            return jsonify({"error": "æŸ¥è¯¢è¯ä¸èƒ½ä¸ºç©º"}), 400

        must_conditions = [
            {"term": {"is_koubei_row": False}},
            {"exists": {"field": "è½¦å‹åç§°.keyword"}}
        ]
        series_name = request.args.get('series_name', type=str, default=None)
        if series_name:
            must_conditions.append({"term": {"è½¦ç³»åç§°.keyword": series_name}})
            print(f"   -> (Autocomplete: é”å®šè½¦ç³» {series_name})")

        es_query = {
            "query": {
                "bool": {
                    "must": must_conditions,
                    "should": [
                        {
                            "multi_match": {
                                "query": q,
                                "fields": ["è½¦å‹åç§°^15", "å“ç‰Œ^5", "åŸºæœ¬ä¿¡æ¯_å‚å•†^5"],
                                "type": "phrase_prefix",
                                "boost": 15.0,
                                "analyzer": "my_custom_search_analyzer"
                            }
                        },
                        {
                            "match_phrase_prefix": {
                                "è½¦ç³»åç§°": {
                                    "query": q,
                                    "boost": 10.0,
                                    "analyzer": "my_custom_search_analyzer"
                                }
                            }
                        },
                        {
                            "wildcard": {
                                "è½¦ç³»åç§°.keyword": {
                                    "value": f"*{q}*",
                                    "boost": 9.0,
                                    "case_insensitive": True
                                }
                            }
                        },
                        {
                            "wildcard": {
                                "è½¦å‹åç§°.keyword": {
                                    "value": f"*{q}*",
                                    "boost": 8.0,
                                    "case_insensitive": True
                                }
                            }
                        },
                        {
                            "multi_match": {
                                "query": q,
                                "fields": ["è½¦å‹åç§°", "è½¦ç³»åç§°", "å“ç‰Œ"],
                                "type": "most_fields",
                                "fuzziness": "AUTO",
                                "boost": 1.0
                            }
                        }
                    ],
                    "minimum_should_match": 1
                }
            },
            "size": 20
        }

        response = es_client.search(index=INDEX_NAME, body=es_query)
        hits = [hit['_source'] for hit in response['hits']['hits']]
        return jsonify(hits)
    except Exception as e:
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500


@app.route('/get-model-config', methods=['GET'])
def get_model_config():
    if not es_client:
        return jsonify({"error": "æ•°æ®åº“æœåŠ¡æœªè¿æ¥"}), 503
    try:
        model_id = request.args.get('model_id', type=str)
        series_name = request.args.get('series_name', type=str)

        if not model_id or not series_name:
            return jsonify({"error": "å¿…é¡»åŒæ—¶æä¾›è½¦å‹ID (model_id) å’Œè½¦ç³»åç§° (series_name)"}), 400

        es_query = {
            "query": {"bool": {
                "must": [{"term": {"è½¦å‹åç§°.keyword": model_id}}, {"term": {"è½¦ç³»åç§°.keyword": series_name}},
                         {"term": {"is_koubei_row": False}}]}},
            "size": 1
        }

        response = es_client.search(index=INDEX_NAME, body=es_query)
        hits = response['hits']['hits']

        if not hits:
            print(f"   -> (è”åˆæŸ¥è¯¢ model='{model_id}', series='{series_name}' å¤±è´¥ï¼Œå°è¯•ä»…ç”¨ model_id æŸ¥è¯¢)")
            es_query_fallback = {
                "query": {
                    "bool": {"must": [{"term": {"è½¦å‹åç§°.keyword": model_id}}, {"term": {"is_koubei_row": False}}]}},
                "size": 1
            }
            response = es_client.search(index=INDEX_NAME, body=es_query_fallback)
            hits = response['hits']['hits']

            if not hits:
                return jsonify({"error": f"æ•°æ®åº“ä¸­æœªæ‰¾åˆ°è½¦å‹ '{model_id}' (å°è¯•äº†è”åˆæŸ¥è¯¢å’Œå•ç‹¬æŸ¥è¯¢)"}), 404
            else:
                print(f"   -> (ä»…ç”¨ model_id æŸ¥è¯¢æˆåŠŸï¼Œè¿”å›ç¬¬ä¸€ä¸ªåŒ¹é…é¡¹)")

        config_data = hits[0]['_source']
        return jsonify(config_data)

    except Exception as e:
        traceback.print_exc()
        return jsonify({"error": f"è·å–è½¦å‹é…ç½®æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {e}"}), 500


def _get_condensed_query(chat_history):
    if not chat_history: return ""
    last_user_message = chat_history[-1]
    last_user_question = last_user_message.get('content', '').strip()
    simple_compare_words = ["æ¯”è¾ƒ", "å¯¹æ¯”", "å¯¹æ¯”ä¸€ä¸‹", "æ¯”å§"]
    is_simple_compare = any(last_user_question.startswith(word) for word in simple_compare_words)
    previous_ai_message = None
    if len(chat_history) > 1:
        for i in range(len(chat_history) - 2, -1, -1):
            if chat_history[i].get('role') == 'assistant':
                previous_ai_message = chat_history[i]
                break
    if is_simple_compare and previous_ai_message:
        previous_ai_response = previous_ai_message.get('content', '')
        mentioned_cars = set()
        pattern = r"(?:ğŸš—|ğŸ’°|ğŸ¯|âœ…|ğŸ†|ğŸ¥ˆ|ğŸ¥‰)\s*\*{0,2}(.*?)(?:\*{0,2}|\s*ï¼ˆ|\n)"
        matches = re.findall(pattern, previous_ai_response)
        for match in matches:
            car_name_match = re.match(r"([\w\s-]+(?:æ–°èƒ½æº)?(?:\s*[\w\d]+(?:æ¬¾|å‹))?)",
                                      match.strip().replace("é¦–é€‰æ¨èï¼š", "").replace("å¤‡é€‰æ¨èï¼š", "").replace(
                                          "è¶…å€¼ä¹‹é€‰ï¼š", "").replace("é¦–æ¨è½¦å‹ï¼š", "").replace("å¤‡é€‰æ¨èï¼š", ""))
            if car_name_match:
                car_name = car_name_match.group(1).strip()
                if len(car_name) > 1 and "ç‰ˆ" not in car_name[-2:] and "æ¬¾" not in car_name[-2:]:
                    mentioned_cars.add(car_name)
        if mentioned_cars:
            extracted_names = " ".join(mentioned_cars)
            rewritten_query = f"å¯¹æ¯” {extracted_names}"
            print(f"   -> (æŸ¥è¯¢é‡å†™: æ£€æµ‹åˆ°æç®€å¯¹æ¯”æé—®ï¼Œå¼ºåˆ¶ç»“åˆä¸Šä¸€è½®å†…å®¹: {rewritten_query})")
            return rewritten_query
        else:
            print(f"   -> (æŸ¥è¯¢é‡å†™: æ£€æµ‹åˆ°æç®€å¯¹æ¯”æé—®ï¼Œä½†æœªèƒ½ä»ä¸Šä¸€è½®æå–è½¦å‹ï¼Œé€€å›åŸå§‹æŸ¥è¯¢: {last_user_question})")
    comparison_keywords = ["å“ªä¸ª", "è¯¦ç»†", "åŒºåˆ«", "è¯´è¯´", "ä»‹ç»ä¸‹"]
    is_vague_follow_up = len(last_user_question) < 10 and any(kw in last_user_question for kw in comparison_keywords)
    if is_vague_follow_up and previous_ai_message:
        previous_ai_response = previous_ai_message.get('content', '')
        mentioned_cars = set()
        pattern = r"(?:ğŸš—|ğŸ’°|ğŸ¯|âœ…|ğŸ†|ğŸ¥ˆ|ğŸ¥‰)\s*\*{0,2}(.*?)(?:\*{0,2}|\s*ï¼ˆ|\n)"
        matches = re.findall(pattern, previous_ai_response)
        for match in matches:
            car_name_match = re.match(r"([\w\s-]+(?:æ–°èƒ½æº)?(?:\s*[\w\d]+(?:æ¬¾|å‹))?)",
                                      match.strip().replace("é¦–é€‰æ¨èï¼š", "").replace("å¤‡é€‰æ¨èï¼š", "").replace(
                                          "è¶…å€¼ä¹‹é€‰ï¼š", "").replace("é¦–æ¨è½¦å‹ï¼š", "").replace("å¤‡é€‰æ¨èï¼š", ""))
            if car_name_match:
                car_name = car_name_match.group(1).strip()
                if len(car_name) > 1 and "ç‰ˆ" not in car_name[-2:] and "æ¬¾" not in car_name[-2:]:
                    mentioned_cars.add(car_name)
        if mentioned_cars:
            extracted_names = " ".join(mentioned_cars)
            rewritten_query = f"{extracted_names} {last_user_question}"
            print(f"   -> (æŸ¥è¯¢é‡å†™: æ£€æµ‹åˆ°æ¨¡ç³Šåç»­æé—®ï¼Œç»“åˆä¸Šä¸€è½®å†…å®¹: {rewritten_query})")
            return rewritten_query
        else:
            print(f"   -> (æŸ¥è¯¢é‡å†™: æ£€æµ‹åˆ°æ¨¡ç³Šåç»­æé—®ï¼Œä½†æœªèƒ½ä»ä¸Šä¸€è½®æå–è½¦å‹ï¼Œé€€å›åŸå§‹æŸ¥è¯¢: {last_user_question})")
    if not llm_client:
        if len(chat_history) > 2:
            for i in range(len(chat_history) - 3, -1, -1):
                if chat_history[i].get('role') == 'user':
                    prev_user_q = chat_history[i].get('content', '')
                    merged_q = f"{prev_user_q} {last_user_question}"
                    print(f"   -> (æŸ¥è¯¢é‡å†™: æ— LLMï¼Œç®€å•åˆå¹¶: {merged_q})")
                    return merged_q
        print(f"   -> (æŸ¥è¯¢é‡å†™: æ— LLMï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢: {last_user_question})")
        return last_user_question
    history_str = ""
    for msg in chat_history:
        if msg.get('role') == 'assistant' and "è¯·å‘Šè¯‰æˆ‘æ‚¨çš„è´­è½¦éœ€æ±‚" in msg.get('content', ''):
            continue
        role = "ç”¨æˆ·" if msg.get('role') == 'user' else "AI"
        history_str += f"{role}: {msg.get('content')}\n"

    is_seemingly_complete = (
            len(chat_history) <= 2 or \
            ("ä¸‡" in last_user_question and (
                    "è½¦" in last_user_question or "SUV" in last_user_question or "MPV" in last_user_question)) or \
            (("å¯¹æ¯”" in last_user_question or "æ¯”è¾ƒ" in last_user_question) and len(last_user_question) > 5)
    )
    if is_seemingly_complete and not is_vague_follow_up and not is_simple_compare:
        print(f"   -> (æŸ¥è¯¢é‡å†™: çœ‹ä¼¼å®Œæ•´æŸ¥è¯¢ï¼Œè·³è¿‡LLMï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢: {last_user_question})")
        return last_user_question

    system_prompt = """ä½ æ˜¯ä¸€ä¸ªæŸ¥è¯¢é‡å†™åŠ©æ‰‹ã€‚ä½ çš„å”¯ä¸€ä»»åŠ¡æ˜¯é˜…è¯»èŠå¤©è®°å½•å’Œç”¨æˆ·çš„æœ€æ–°é—®é¢˜ï¼Œ
ç„¶åå°†æœ€æ–°çš„é—®é¢˜æ”¹å†™æˆä¸€ä¸ªç‹¬ç«‹çš„ã€å®Œæ•´çš„æŸ¥è¯¢ï¼Œä»¥ä¾¿äºåœ¨æ•°æ®åº“ä¸­æœç´¢ã€‚
**è§„åˆ™:**
1. ä¿æŒåŸå§‹æŸ¥è¯¢ä¸­çš„æ‰€æœ‰å…³é”®çº¦æŸï¼ˆå¦‚ä»·æ ¼ã€å“ç‰Œã€è½¦å‹ã€åŠ¨åŠ›ç±»å‹ã€è½¦èº«ç±»å‹ç­‰ï¼‰ã€‚
2. ä»èŠå¤©è®°å½•ä¸­ç»§æ‰¿ä¸Šä¸‹æ–‡ï¼ˆå¦‚ "20ä¸‡å·¦å³", "SUV"ï¼‰ï¼Œå¹¶å°†å…¶ä¸æœ€æ–°é—®é¢˜ï¼ˆå¦‚ "ç‡ƒæ²¹çš„å‘¢"ï¼‰åˆå¹¶ã€‚
3. åªè¿”å›æ”¹å†™åçš„æŸ¥è¯¢ï¼Œä¸è¦æœ‰ä»»ä½•è§£é‡Šã€‚
**ç¤ºä¾‹ 1:**
èŠå¤©è®°å½•:
ç”¨æˆ·: 20ä¸‡å·¦å³çš„suv
AI: å¥½çš„ï¼Œæˆ‘ä¸ºæ‚¨æ‰¾åˆ°...
ç”¨æˆ·: ç‡ƒæ²¹çš„å‘¢
æ”¹å†™åçš„æŸ¥è¯¢: 20ä¸‡å·¦å³çš„ç‡ƒæ²¹suv
**ç¤ºä¾‹ 2:**
èŠå¤©è®°å½•:
ç”¨æˆ·: æ¨èå‡ æ¬¾å¥¥è¿ª
AI: å¥½çš„ï¼Œæ‚¨å¯¹ä»·æ ¼...
ç”¨æˆ·: 50ä¸‡ä»¥å†…ï¼Œè½¿è½¦
æ”¹å†™åçš„æŸ¥è¯¢: 50ä¸‡ä»¥å†…çš„å¥¥è¿ªè½¿è½¦
"""
    user_prompt = f"èŠå¤©è®°å½•:\n{history_str}\næ”¹å†™åçš„æŸ¥è¯¢:"
    try:
        response = llm_client.chat.completions.create(
            model=DEEPSEEK_MODEL_NAME,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.0,
            max_tokens=100
        )
        condensed_query = response.choices[0].message.content.strip()
        if not condensed_query:
            print(f"   -> (æŸ¥è¯¢é‡å†™: LLM è¿”å›ç©ºï¼Œé€€å›åŸå§‹æŸ¥è¯¢: {last_user_question})")
            return last_user_question
        if condensed_query in simple_compare_words:
            print(f"   -> (æŸ¥è¯¢é‡å†™: LLMä»…è¿”å›æ¯”è¾ƒè¯ï¼Œå¯èƒ½æœªç†è§£ä¸Šä¸‹æ–‡ï¼Œé€€å›åŸå§‹æŸ¥è¯¢: {last_user_question})")
            return last_user_question
        print(f"   -> (æŸ¥è¯¢é‡å†™: LLMæ”¹å†™ç»“æœ: {condensed_query})")
        return condensed_query
    except Exception as e:
        print(f"   -> (æŸ¥è¯¢é‡å†™: LLMè°ƒç”¨å¤±è´¥: {e}ï¼Œé€€å›åŸå§‹æŸ¥è¯¢: {last_user_question})")
        return last_user_question


def create_parser_prompt(user_prompt):
    feature_field_mapping_docs = ""
    if FEATURE_MAPPING:
        feature_field_mapping_docs += "**ç‰¹æ€§->å­—æ®µæ˜ å°„è¡¨ (ç”¨äºæŸ¥æ‰¾å­—æ®µ):**\n"
        for feature, mapping in FEATURE_MAPPING.items():
            fields_str = ", ".join(mapping.get("fields", []))
            aliases = [feature]
            for term in mapping.get("search_terms", []):
                if term not in ["â—", "â—‹", "æ ‡é…", "é€‰é…", "æ˜¯", "æœ‰"] and len(term) > 1 and term != feature:
                    aliases.append(term)
            aliases_str = ", ".join(list(set(aliases)))
            feature_field_mapping_docs += f"* ç”¨æˆ·è¯´ '{aliases_str}' -> `fields`: [{fields_str}]\n"
        feature_field_mapping_docs += "\n"
    else:
        feature_field_mapping_docs = """
**ç‰¹æ€§->å­—æ®µæ˜ å°„è¡¨ (å›é€€):**
* "å››é©±" -> `fields`: ["åº•ç›˜è½¬å‘_é©±åŠ¨å½¢å¼", "è½¦å‹åç§°.keyword"]
* "çŸ©é˜µå¤§ç¯" -> `fields`: ["ç¯å…‰åŠŸèƒ½_ç¯å…‰ç‰¹è‰²åŠŸèƒ½", "ç¯å…‰åŠŸèƒ½_è¿œå…‰ç¯å…‰æº","ç¯å…‰åŠŸèƒ½_è¿‘å…‰ç¯å…‰æº"]
* "åº§æ¤…é€šé£" -> `fields`: ["åº§æ¤…é…ç½®_ç¬¬ä¸€æ’åº§æ¤…åŠŸèƒ½", "åº§æ¤…é…ç½®_ç¬¬äºŒæ’åº§æ¤…åŠŸèƒ½"]
"""
    return f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ±½è½¦æœç´¢å¼•æ“æŸ¥è¯¢åˆ†æå¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æç”¨æˆ·çš„è‡ªç„¶è¯­è¨€è¾“å…¥ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºä¸€ä¸ªç»“æ„åŒ–çš„JSONå¯¹è±¡ï¼Œä»¥ä¾¿äºåç»­çš„æ•°æ®åº“æŸ¥è¯¢ã€‚
**JSONç»“æ„å®šä¹‰:**
{{
  "brand": ["å“ç‰Œå"] | null,
  "manufacturer": ["å‚å•†å"] | null,
  "series": ["è½¦ç³»å"] | null,
  "price_min": (æ•°å­—, å•ä½:ä¸‡) | null,
  "price_max": (æ•°å­—, å•ä½:ä¸‡) | null,
  "power_type": ["çº¯ç”µ", ...] | null,
  "body_type": ["SUV", ...] | null,
  "feature_filters": [
    {{ "query": "ç”¨æˆ·æ„å›¾æ ¸å¿ƒè¯", "fields": ["æ•°æ®åº“å­—æ®µå1", ...] }}
  ] | null,
  "keywords_for_llm": "ç”¨æˆ·åŸå§‹æé—®çš„ç²¾ç®€ç‰ˆ"
}}
**è§£æè§„åˆ™:**
1.  **[!] æå–æ ¸å¿ƒè½¦è¾†ç±»å‹ (å¿…é¡»ä¼˜å…ˆå¤„ç†!)**:
    * **åŠ¨åŠ›ç±»å‹ (power_type)**:
        * ä»ç”¨æˆ·è¾“å…¥ä¸­è¯†åˆ«æ˜ç¡®çš„åŠ¨åŠ›ç±»å‹ï¼Œå¦‚ "ç‡ƒæ²¹", "çº¯ç”µ", "æ’ç”µæ··åŠ¨", "å¢ç¨‹å¼" ç­‰ã€‚
        * **[!] ç‰¹æ®Šæ˜ å°„**: å¦‚æœç”¨æˆ·æ˜ç¡®æåˆ° **"æ–°èƒ½æº"** å¹¶ä¸”æ²¡æœ‰æŒ‡å®šæ›´å…·ä½“çš„ç±»å‹ï¼ˆå¦‚çº¯ç”µ/æ’æ··ï¼‰ï¼Œ**å¿…é¡»**å°† `power_type` è®¾ç½®ä¸º `["çº¯ç”µ", "æ’ç”µæ··åŠ¨", "å¢ç¨‹å¼"]`ã€‚
        * å¦‚æœç”¨æˆ·åŒæ—¶æåˆ°äº† "æ–°èƒ½æº" å’Œå…·ä½“ç±»å‹ï¼ˆå¦‚ "æ–°èƒ½æºçº¯ç”µ"ï¼‰ï¼Œåˆ™ä¼˜å…ˆä½¿ç”¨å…·ä½“ç±»å‹ï¼ˆ`["çº¯ç”µ"]`ï¼‰ã€‚
        * å¦‚æœæåˆ°äº†æ˜ç¡®ç±»å‹ï¼Œ**å¿…é¡»**å¡«å…¥ `power_type` åˆ—è¡¨ã€‚
    * **è½¦èº«ç±»å‹ (body_type)**: ä»ç”¨æˆ·è¾“å…¥ä¸­è¯†åˆ«æ˜ç¡®çš„è½¦èº«ç±»å‹ï¼Œå¦‚ "SUV", "è½¿è½¦", "MPV" ç­‰ã€‚å¦‚æœæåˆ°ï¼Œ**å¿…é¡»**å¡«å…¥ `body_type` åˆ—è¡¨ã€‚
2.  **å“ç‰Œ (brand) / å‚å•† (manufacturer) / è½¦ç³» (series) è¯†åˆ« (é‡è¦!)**:
    * **ç›®æ ‡**: æ­£ç¡®åŒºåˆ†å¹¶æå– `brand`, `manufacturer`, `series`ã€‚
    * **è½¦ç³»è¯†åˆ«**: ä¼˜å…ˆè¯†åˆ«**è½¦ç³»å** (ä¾‹å¦‚ "Model 3", "Model Y", "A4L", "5ç³»", "é—®ç•ŒM7", "æ™ºç•ŒR7")ã€‚å¦‚æœè¯†åˆ«åˆ°ï¼Œå¡«å…¥ `series` åˆ—è¡¨ã€‚
    * **å“ç‰Œ/å‚å•†è¯†åˆ«**: åŒæ—¶è¯†åˆ«**å“ç‰Œå** (ä¾‹å¦‚ "ç‰¹æ–¯æ‹‰", "å®é©¬", "å¥¥è¿ª", "é¸¿è’™æ™ºè¡Œ") æˆ–**å‚å•†å** (ä¾‹å¦‚ "é—®ç•Œ", "æ™ºç•Œ", "äº«ç•Œ")ã€‚
    * **ç»„åˆå¤„ç† (å…³é”®!)**:
        * **"ç‰¹æ–¯æ‹‰Model 3"**: **å¿…é¡»**è§£æä¸º `brand: ["ç‰¹æ–¯æ‹‰"], series: ["Model 3"]`ã€‚
        * **"é—®ç•ŒM7"**: **å¿…é¡»**è§£æä¸º `manufacturer: ["é—®ç•Œ"], series: ["é—®ç•ŒM7"], brand: ["é¸¿è’™æ™ºè¡Œ"]` (å› ä¸ºé—®ç•Œæ˜¯å‚å•†ï¼Œéš¶å±é¸¿è’™æ™ºè¡Œå“ç‰Œï¼Œè½¦ç³»æ˜¯é—®ç•ŒM7)ã€‚
        * **"å¥¥è¿ªA4L"**: **å¿…é¡»**è§£æä¸º `brand: ["å¥¥è¿ª"], series: ["A4L"]` (A4Læ˜¯è½¦ç³»)ã€‚
        * **"å®é©¬5ç³»"**: **å¿…é¡»**è§£æä¸º `brand: ["å®é©¬"], series: ["5ç³»"]`ã€‚
    * **å•ç‹¬æåŠ**:
        * **"ç‰¹æ–¯æ‹‰"**: è§£æä¸º `brand: ["ç‰¹æ–¯æ‹‰"]`ã€‚
        * **"é—®ç•Œ"**: è§£æä¸º `manufacturer: ["é—®ç•Œ"], brand: ["é¸¿è’™æ™ºè¡Œ"]`ã€‚
        * **"é¸¿è’™æ™ºè¡Œ"**: è§£æä¸º `brand: ["é¸¿è’™æ™ºè¡Œ"]`ã€‚
        * **"Model 3"**: è§£æä¸º `series: ["Model 3"]`ã€‚
        * **"M7"**: è§£æä¸º `series: ["é—®ç•ŒM7"]` (éœ€è¦æ˜ å°„)ã€‚
    * **è½¦å‹å·æ˜ å°„**: å°†å¸¸è§çš„ç®€å†™æˆ–å‹å·ï¼ˆå¦‚ "M7", "3", "530Li", "A4L"ï¼‰æ˜ å°„åˆ°**å®Œæ•´è½¦ç³»å**å¡«å…¥ `series` (ä¾‹å¦‚ "M7" -> `series: ["é—®ç•ŒM7"]`, "3" (å¦‚æœä¸Šä¸‹æ–‡æ˜¯ç‰¹æ–¯æ‹‰) -> `series: ["Model 3"]`, "530Li" -> `series: ["5ç³»"]`)ã€‚
    * **ä¼˜å…ˆçº§**: ä»¥æœ€ç²¾ç¡®çš„è¯†åˆ«ï¼ˆé€šå¸¸æ˜¯è½¦ç³»ï¼‰ä¸ºä¸»ã€‚å¦‚æœè¯†åˆ«å‡º `series`ï¼Œ`brand` å’Œ `manufacturer` ä½œä¸ºè¡¥å……ã€‚
3.  **ä»·æ ¼**:
    * "XXä¸‡å·¦å³" -> å¿…é¡»è§£æä¸ºä¸€ä¸ªåŒºé—´ï¼Œä¾‹å¦‚ "30ä¸‡å·¦å³" è§£æä¸º `price_min: 27, price_max: 33` (ä¸Šä¸‹æµ®åŠ¨10%)ã€‚
    * "XXåˆ°YYä¸‡" -> è§£æä¸º `price_min: XX, price_max: YY`ã€‚
    * "XXä¸‡ä»¥å†…/ä»¥ä¸‹" -> è§£æä¸º `price_max: XX`ã€‚
    * "XXä¸‡ä»¥ä¸Š" -> è§£æä¸º `price_min: XX`ã€‚
    * æ­£å¸¸è§£ææ•°å­—ã€‚
4.  **ç‰¹æ€§è¿‡æ»¤å™¨ (feature_filters)**:
    * è¯†åˆ«ç”¨æˆ·æåˆ°çš„æ ¸å¿ƒé…ç½®æˆ–ç‰¹æ€§ã€‚**å¿…é¡»**å¿½ç•¥ "æ ‡é…"ã€"é€‰é…"ã€"å¸¦ä¸å¸¦"ã€"æœ‰æ²¡æœ‰"ã€"æ˜¯å¦" ç­‰ä¿®é¥°è¯ï¼Œåªæå–**æ ¸å¿ƒç‰¹æ€§è¯**ï¼ˆä¾‹å¦‚ "åº§æ¤…é€šé£", "å››é©±", "çŸ©é˜µå¤§ç¯"ï¼‰ä½œä¸º `query` çš„å€¼ã€‚
    * æ ¹æ®**ä¸‹é¢çš„æ˜ å°„è¡¨**ï¼Œæ‰¾åˆ°è¿™ä¸ªæ ¸å¿ƒç‰¹æ€§è¯å¯¹åº”çš„**æ•°æ®åº“å­—æ®µ**ï¼Œå¡«å…¥ `fields` åˆ—è¡¨ã€‚
    {feature_field_mapping_docs}
5.  **keywords_for_llm**: ç”¨æˆ·åŸå§‹æé—®ç²¾ç®€ç‰ˆã€‚
6.  **åªè¿”å›JSON**ã€‚
---
**ç¤ºä¾‹:**
- **è¾“å…¥**: "å¯¹æ¯”ä¸€ä¸‹å¥¥è¿ªA4Lå’Œå®é©¬530Li"
- **è¾“å‡º**:
{{
  "brand": ["å¥¥è¿ª", "å®é©¬"],
  "manufacturer": null,
  "series": ["A4L", "5ç³»"],
  "price_min": null,
  "price_max": null,
  "power_type": null,
  "body_type": null,
  "feature_filters": null,
  "keywords_for_llm": "å¯¹æ¯”å¥¥è¿ªA4Lå’Œå®é©¬530Li"
}}
---
è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°è§„åˆ™ï¼Œè§£æä»¥ä¸‹ç”¨æˆ·è¾“å…¥ï¼š
"{user_prompt}"
"""


def _create_hybrid_query(names_list, keyword_field, text_field):
    if not names_list: return None
    hybrid_should_clauses = []
    for name in names_list:
        if not name: continue
        hybrid_should_clauses.append(
            {"prefix": {keyword_field: {"value": name, "boost": 10.0, "case_insensitive": True}}})
        hybrid_should_clauses.append({"match": {text_field: {"query": name, "boost": 5.0}}})
    if not hybrid_should_clauses: return None
    return {"bool": {"should": hybrid_should_clauses, "minimum_should_match": 1}}


def clean_es_result(doc):
    cleaned_doc = {}
    if not isinstance(doc, dict):
        return doc
    for key, value in doc.items():
        new_key = key.replace(".keyword", "")
        if isinstance(value, dict):
            cleaned_doc[new_key] = clean_es_result(value)
        elif isinstance(value, list):
            cleaned_doc[new_key] = [clean_es_result(item) if isinstance(item, dict) else item for item in value]
        else:
            cleaned_doc[new_key] = value
    return cleaned_doc


@app.route('/ai_chat', methods=['POST'])
def ai_chat():
    if not llm_client: return jsonify({"error": "AIæœåŠ¡æœªé…ç½®æˆ–ä¸å¯ç”¨"}), 503
    if not es_client: return jsonify({"error": "æ•°æ®åº“æœåŠ¡æœªè¿æ¥"}), 503
    data = request.json
    chat_history = data.get('history', [])
    if not chat_history:
        return jsonify({"error": "èŠå¤©å†…å®¹ä¸èƒ½ä¸ºç©º"}), 400
    current_prompt = chat_history[-1].get('content', '')
    if not current_prompt:
        return jsonify({"error": "æœ€æ–°æé—®ä¸èƒ½ä¸ºç©º"}), 400
    known_series = data.get('known_series')
    known_model = data.get('known_model')
    has_known_series = bool(known_series and known_series.strip())
    has_known_model = bool(known_model and known_model.strip())
    is_first_message = len(chat_history) == 1
    # è¯†åˆ«æ¨¡ç³Šçš„å¯¹æ¯”æˆ–åç»­æé—® (ä¾‹å¦‚: "å’Œä»–æ¯”", "å’Œ5ç³»æ¯”", "æ€ä¹ˆæ ·?", "å‘¢?")
    vague_starters = ["å’Œä»–", "å’Œå®ƒ", "å¯¹æ¯”", "æ¯”è¾ƒ", "æ¯”ä¸€æ¯”", "è·Ÿå®ƒ", "è·Ÿ", "é‚£"]
    vague_enders = ["å‘¢", "å‘¢ï¼Ÿ", "å’‹æ ·", "å’‹æ ·ï¼Ÿ", "æ€ä¹ˆæ ·", "æ€ä¹ˆæ ·ï¼Ÿ"]

    is_vague_comparison = any(current_prompt.startswith(term) for term in vague_starters)
    is_vague_follow_up = any(current_prompt.endswith(term) for term in vague_enders) and len(
        current_prompt) < 25

    if (has_known_series or has_known_model) and is_first_message and (is_vague_comparison or is_vague_follow_up):
        context_to_add = known_model if has_known_model else known_series

        chat_history.insert(0, {"role": "user", "content": context_to_add})
        print(f"   -> (AI Chat: æ£€æµ‹åˆ°æ¨¡ç³Šæé—®ï¼Œè‡ªåŠ¨æ³¨å…¥ä¸Šä¸‹æ–‡ '{context_to_add}')")

    print(f"\n\n--- [AI CHAT DEBUG] ---")
    print(f"1. æ¥æ”¶åˆ°èŠå¤©è®°å½•ï¼Œæœ€æ–°æé—®: {current_prompt}")
    if has_known_series: print(f"   -> å·²çŸ¥è½¦ç³» (Known Series): {known_series}")
    if has_known_model: print(f"   -> å·²çŸ¥è½¦å‹ (Known Model): {known_model}")
    try:
        condensed_query = _get_condensed_query(chat_history)
        applied_feature_filter = False
        requested_fields_set = set()
        parser_prompt_text = create_parser_prompt(condensed_query)
        parsed_params = {}
        try:
            llm_response_parser = llm_client.chat.completions.create(
                model=DEEPSEEK_MODEL_NAME,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªåªè¿”å›JSONçš„æŸ¥è¯¢è§£æå™¨ã€‚"},
                    {"role": "user", "content": parser_prompt_text}
                ],
                temperature=0.0
            )
            response_text = llm_response_parser.choices[0].message.content
            match = re.search(r"\{.*\}", response_text, re.DOTALL)
            if not match: raise ValueError("AIæœªèƒ½è¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼")
            parsed_params = json.loads(match.group(0))
            print(f"2. AIæ„å›¾è§£æ (JSON): \n{json.dumps(parsed_params, indent=2, ensure_ascii=False)}")
        except Exception as e:
            print(f"2. AIæ„å›¾è§£æå¤±è´¥: {e}ã€‚é€€å›åˆ°å…³é”®è¯åŒ¹é…ã€‚")
            traceback.print_exc()
            parsed_params = {"keywords_for_llm": condensed_query}
        param_must_conditions = [{"term": {"is_koubei_row": False}}]
        param_should_conditions = []
        all_series_for_query = []
        if has_known_series:
            param_must_conditions.append({"term": {"è½¦ç³»åç§°.keyword": known_series}})
            print(f"   -> (å¼ºåˆ¶èŒƒå›´: ç²¾ç¡®åŒ¹é… known_series = {known_series})")
            all_series_for_query.append(known_series)
            if has_known_model:
                param_must_conditions.append({"term": {"è½¦å‹åç§°.keyword": known_model}})
                print(f"   -> (å¼ºåˆ¶èŒƒå›´: ç²¾ç¡®åŒ¹é… known_model = {known_model})")
        ai_parsed_series = parsed_params.get("series", [])
        ai_parsed_brands = parsed_params.get("brand", [])
        ai_parsed_mfgs = parsed_params.get("manufacturer", [])
        is_comparison = has_known_series and ai_parsed_series and any(
            s.lower() != known_series.lower() for s in ai_parsed_series)
        if is_comparison:
            comparison_targets = [s for s in ai_parsed_series if s.lower() != known_series.lower()]
            if comparison_targets:
                all_series_for_query.extend(comparison_targets)
                comparison_query = _create_hybrid_query(comparison_targets, "è½¦ç³»åç§°.keyword", "è½¦ç³»åç§°")
                param_must_conditions = [
                    cond for cond in param_must_conditions if (
                            cond.get("term", {}).get("è½¦ç³»åç§°.keyword") != known_series and
                            (not known_model or cond.get("term", {}).get("è½¦å‹åç§°.keyword") != known_model)
                    )
                ]
                comparison_should_clauses = []
                if has_known_model:
                    model_query = _create_hybrid_query([known_model], "è½¦å‹åç§°.keyword", "è½¦å‹åç§°")
                    if model_query:
                        comparison_should_clauses.append(model_query)
                if has_known_series:
                    series_query = _create_hybrid_query([known_series], "è½¦ç³»åç§°.keyword", "è½¦ç³»åç§°")
                    if series_query:
                        comparison_should_clauses.append(series_query)
                if comparison_query:
                    comparison_should_clauses.append(comparison_query)
                if comparison_should_clauses:
                    param_must_conditions.append({
                        "bool": {
                            "should": comparison_should_clauses,
                            "minimum_should_match": 1
                        }
                    })
                    print(f"   -> (å¯¹æ¯”èŒƒå›´: å·²é‡æ„æŸ¥è¯¢ä»¥åŒ…å« {known_series}, {known_model} å’Œ {comparison_targets})")
        elif not has_known_series:
            print(f"   -> (æ— ä¸Šä¸‹æ–‡ï¼Œä½¿ç”¨AIè§£æç»“æœè¿›è¡Œæ¨¡ç³ŠæŸ¥æ‰¾)")
            if ai_parsed_series:
                all_series_for_query = ai_parsed_series
                hybrid_series_query = _create_hybrid_query(ai_parsed_series, "è½¦ç³»åç§°.keyword", "è½¦ç³»åç§°")
                if hybrid_series_query:
                    param_must_conditions.append(hybrid_series_query)
                    print(f"   -> (æ·»åŠ  AI è§£æçš„è½¦ç³»(hybrid)æ¡ä»¶: {ai_parsed_series})")
            elif ai_parsed_brands:
                hybrid_brand_query = _create_hybrid_query(ai_parsed_brands, "å“ç‰Œ.keyword", "å“ç‰Œ")
                if hybrid_brand_query:
                    param_must_conditions.append(hybrid_brand_query)
                    print(f"   -> (æ·»åŠ  AI è§£æçš„å“ç‰Œ(hybrid)æ¡ä»¶: {ai_parsed_brands})")
            elif ai_parsed_mfgs:
                hybrid_mfg_query = _create_hybrid_query(ai_parsed_mfgs, "åŸºæœ¬ä¿¡æ¯_å‚å•†.keyword", "åŸºæœ¬ä¿¡æ¯_å‚å•†")
                if hybrid_mfg_query:
                    param_must_conditions.append(hybrid_mfg_query)
                    print(f"   -> (æ·»åŠ  AI è§£æçš„å‚å•†(hybrid)æ¡ä»¶: {ai_parsed_mfgs})")
        feature_filters = parsed_params.get("feature_filters", [])
        if feature_filters:
            feature_should_clauses = []
            for f_filter in feature_filters:
                user_intent_query = f_filter.get("query")
                ai_suggested_fields = f_filter.get("fields", [])
                search_terms = []
                target_fields = ai_suggested_fields
                if FEATURE_MAPPING and user_intent_query and user_intent_query in FEATURE_MAPPING:
                    mapping = FEATURE_MAPPING[user_intent_query]
                    base_search_terms = mapping.get("search_terms", [user_intent_query])
                    search_terms = list(set(base_search_terms + ["æ ‡é…", "â—", "é€‰é…", "â—‹"]))
                    target_fields = mapping.get("fields", ai_suggested_fields)
                    print_prefix = f"ç‰¹æ€§è¿‡æ»¤å™¨(æ˜ å°„): '{user_intent_query}'"
                elif user_intent_query and ai_suggested_fields:
                    search_terms = [user_intent_query, "æ ‡é…", "â—", "é€‰é…", "â—‹"]
                    target_fields = ai_suggested_fields
                    print_prefix = f"ç‰¹æ€§è¿‡æ»¤å™¨(å›é€€): '{user_intent_query}'"
                else:
                    print(f"   -> (è­¦å‘Š: AI è§£æçš„ç‰¹æ€§è¿‡æ»¤å™¨æ— æ•ˆ: {f_filter})")
                    continue
                expanded_target_fields = set(target_fields)
                expanded_target_fields.add("è½¦å‹åç§°")
                expanded_target_fields.add("è½¦ç³»åç§°")
                if expanded_target_fields:
                    requested_fields_set.update(expanded_target_fields)
                if search_terms and expanded_target_fields:
                    print(f"   -> (ç‰¹æ€§æœç´¢è¯ (å·²æ‰©å±•): {search_terms})")
                    wildcard_should_clauses_for_feature = []
                    for field in expanded_target_fields:
                        text_field = field.replace(".keyword", "")
                        keyword_field = field if ".keyword" in field else f"{field}.keyword"
                        for term_val in search_terms:
                            wildcard_should_clauses_for_feature.append({
                                "wildcard": {
                                    keyword_field: {"value": f"*{term_val}*", "case_insensitive": True, "boost": 2.0}}
                            })
                            wildcard_should_clauses_for_feature.append({
                                "match": {text_field: {"query": term_val, "boost": 1.0}}
                            })
                    if wildcard_should_clauses_for_feature:
                        feature_should_clauses.append({
                            "bool": {
                                "should": wildcard_should_clauses_for_feature,
                                "minimum_should_match": 1
                            }
                        })
                        print(
                            f"   -> ({print_prefix} -> æ·»åŠ ç‰¹æ€§åŒ¹é…(should): {search_terms} in {list(expanded_target_fields)})")
                        applied_feature_filter = True
            if feature_should_clauses:
                param_should_conditions.extend(feature_should_clauses)
        price_min_filter = parsed_params.get("price_min")
        price_max_filter = parsed_params.get("price_max")
        if price_min_filter or price_max_filter:
            price_query = {}
            if price_min_filter: price_query["gte"] = price_min_filter
            if price_max_filter: price_query["lte"] = price_max_filter
            param_must_conditions.append({"range": {"price_numeric": price_query}})
            print(f"   -> (æ·»åŠ ä»·æ ¼æ¡ä»¶(must): {price_query})")
        if parsed_params.get("power_type"):
            param_must_conditions.append({"terms": {"åŠ¨åŠ›ç±»å‹": parsed_params.get("power_type")}})
            print(f"   -> (æ·»åŠ åŠ¨åŠ›æ¡ä»¶(must): {parsed_params.get('power_type')})")
        if parsed_params.get("body_type"):
            param_must_conditions.append({"terms": {"è½¦èº«ç±»å‹": parsed_params.get("body_type")}})
            print(f"   -> (æ·»åŠ è½¦èº«æ¡ä»¶(must): {parsed_params.get('body_type')})")
        keywords_for_llm = parsed_params.get("keywords_for_llm", "")
        if keywords_for_llm and not applied_feature_filter and not is_comparison:
            param_should_conditions.append(
                {"multi_match": {
                    "query": keywords_for_llm,
                    "fields": ["å“ç‰Œ", "è½¦ç³»åç§°", "è½¦å‹åç§°^2"],
                    "type": "best_fields",
                    "fuzziness": "AUTO",
                    "boost": 1.0
                }}
            )
            print(f"   -> (æ·»åŠ  LLM å…³é”®è¯(should): {keywords_for_llm})")
        elif is_comparison:
            print(f"   -> (å¯¹æ¯”æŸ¥è¯¢ï¼Œå·²è·³è¿‡ LLM å…³é”®è¯(should) ä»¥é˜²æ­¢æ’åæ±¡æŸ“)")
        min_should_match = 0
        if param_should_conditions:
            print(f"   -> (æœ‰ {len(param_should_conditions)} ä¸ª should æ¡ä»¶ï¼Œä»…ç”¨äºæåˆ†ï¼Œminimum_should_match=0)")
        else:
            print(f"   -> (æ—  should æ¡ä»¶ï¼Œminimum_should_match=0)")
        price_functions = []
        if price_max_filter and not price_min_filter:
            price_functions.append({"gauss": {
                "price_numeric": {"origin": price_max_filter, "scale": price_max_filter / 2.5,
                                  "offset": price_max_filter / 5, "decay": 0.5}}, "weight": 2})
            print(f"   -> (ä»·æ ¼ä¼˜åŒ–ï¼šæå‡æ¥è¿‘ {price_max_filter}ä¸‡ çš„è½¦å‹å¾—åˆ†)")
        elif price_min_filter and not price_max_filter:
            price_functions.append({"gauss": {
                "price_numeric": {"origin": price_min_filter, "scale": price_min_filter * 2,
                                  "offset": price_min_filter / 5, "decay": 0.5}}, "weight": 2})
            print(f"   -> (ä»·æ ¼ä¼˜åŒ–ï¼šæå‡é«˜äº {price_min_filter}ä¸‡ çš„è½¦å‹å¾—åˆ†)")
        elif price_min_filter and price_max_filter:
            origin_price = (price_min_filter + price_max_filter) / 2
            scale_price = (
                                  price_max_filter - price_min_filter) / 2 if price_max_filter > price_min_filter else price_min_filter / 2
            price_functions.append({"gauss": {"price_numeric": {"origin": origin_price, "scale": max(scale_price, 5),
                                                                "offset": max(scale_price, 5) / 2, "decay": 0.5}},
                                    "weight": 3})
            print(f"   -> (ä»·æ ¼ä¼˜åŒ–ï¼šæå‡ {price_min_filter}-{price_max_filter}ä¸‡ èŒƒå›´çš„è½¦å‹å¾—åˆ†)")
        base_bool_query = {"bool": {"must": param_must_conditions, "should": param_should_conditions,
                                    "minimum_should_match": min_should_match}}
        fetch_size = 60
        if price_functions:
            param_query = {
                "query": {
                    "function_score": {
                        "query": base_bool_query,
                        "functions": price_functions,
                        "score_mode": "multiply",
                        "boost_mode": "multiply"
                    }
                },
                "size": fetch_size
            }
            print(f"   -> (ä½¿ç”¨ function_score (multiply æ¨¡å¼) è¿›è¡Œä»·æ ¼æåˆ†)")
        else:
            param_query = {"query": base_bool_query, "size": fetch_size}
            print(f"   -> (ä½¿ç”¨åŸºç¡€ bool æŸ¥è¯¢)")
        print(f"3. å‚æ•°åº“ESæŸ¥è¯¢ (Query): \n{json.dumps(param_query, indent=2, ensure_ascii=False)}")
        param_response = es_client.search(index=INDEX_NAME, body=param_query)
        raw_param_results = [hit['_source'] for hit in param_response['hits']['hits']]
        param_results = [clean_es_result(doc) for doc in raw_param_results]
        print(f"4. å‚æ•°åº“ESç»“æœ: å¬å› {len(param_results)} ä¸ªè½¦å‹ (å·²æ¸…ç†)")
        target_series_for_review = []
        if all_series_for_query:
            target_series_for_review = list(set(all_series_for_query))
            print(f"   -> (å£ç¢‘æŸ¥è¯¢ç›®æ ‡è½¦ç³»: {target_series_for_review})")
        else:
            found_series_names = list(set([p.get('è½¦ç³»åç§°') for p in param_results if p.get('è½¦ç³»åç§°')]))
            if found_series_names:
                target_series_for_review = found_series_names
                print(f"   -> (å£ç¢‘åŸºäºå‚æ•°åº“ç»“æœè½¦ç³»: {found_series_names})")
        review_results_raw = []
        if target_series_for_review:
            review_must_conditions = [{"term": {"is_koubei_row": True}}]
            review_scope_should = []
            if has_known_series and known_series in target_series_for_review:
                series_query = _create_hybrid_query([known_series], "è½¦ç³»åç§°.keyword", "è½¦ç³»åç§°")
                if series_query:
                    review_scope_should.append(series_query)
            other_targets = [s for s in target_series_for_review if s != known_series]
            if other_targets:
                hybrid_other_query = _create_hybrid_query(other_targets, "è½¦ç³»åç§°.keyword", "è½¦ç³»åç§°")
                if hybrid_other_query:
                    review_scope_should.append(hybrid_other_query)
            if review_scope_should:
                review_must_conditions.append({"bool": {"should": review_scope_should, "minimum_should_match": 1}})
            review_should_conditions_features = []
            if feature_filters:
                for f_filter in feature_filters:
                    user_intent_query = f_filter.get("query")
                    search_terms_for_review = []
                    if FEATURE_MAPPING and user_intent_query and user_intent_query in FEATURE_MAPPING:
                        search_terms_for_review = FEATURE_MAPPING[user_intent_query].get("search_terms",
                                                                                         [user_intent_query])
                    elif user_intent_query:
                        search_terms_for_review = [user_intent_query]
                    if search_terms_for_review:
                        extended_review_terms = " ".join(
                            list(set(search_terms_for_review + ["æ ‡é…", "â—", "é€‰é…", "â—‹"])))
                        review_should_conditions_features.append(
                            {"match": {"æ‰€æœ‰è¯„ä»·": {"query": extended_review_terms, "boost": 10}}}
                        )
            review_query = {
                "query": {"bool": {"must": review_must_conditions, "should": review_should_conditions_features,
                                   "minimum_should_match": 0}},
                "size": 20
            }
            print(f"5. å£ç¢‘åº“ESæŸ¥è¯¢ (Query): \n{json.dumps(review_query, indent=2, ensure_ascii=False)}")
            review_response = es_client.search(index=INDEX_NAME, body=review_query)
            review_results_raw = [hit['_source'] for hit in review_response['hits']['hits']]
            print(f"6. å£ç¢‘åº“ESç»“æœ: æ‰¾åˆ°äº† {len(review_results_raw)} æ¡ç›¸å…³å£ç¢‘")
        else:
            print("5. æœªç¡®å®šç›®æ ‡è½¦ç³»ï¼Œè·³è¿‡å£ç¢‘åº“ç²¾ç¡®æŸ¥è¯¢ã€‚")
        review_results = [clean_es_result(doc) for doc in review_results_raw]
        context_for_llm = ""
        context_for_llm += "--- è½¦è¾†å‚æ•°èµ„æ–™åº“ (ä¾›ä½ æ¨èæˆ–å¯¹æ¯”çš„è½¦å‹) ---\n"
        raw_recommended_cars_map = {}
        if param_results:
            final_models_for_recommendation = param_results
            for i, res in enumerate(final_models_for_recommendation):
                model_id_for_llm = str(i + 1)
                model_marker = ""
                if has_known_model and res.get('è½¦å‹åç§°') == known_model:
                    model_marker = "ã€ç”¨æˆ·å½“å‰å…³æ³¨è½¦å‹èµ„æ–™ã€‘"
                context_for_llm += f"ã€ID: {model_id_for_llm}ã€‘{model_marker}\n"
                context_for_llm += f"  - å‚å•†: {res.get('åŸºæœ¬ä¿¡æ¯_å‚å•†', 'N/A')}\n"
                context_for_llm += f"  - è½¦ç³»: {res.get('è½¦ç³»åç§°', 'N/A')}\n"
                context_for_llm += f"  - è½¦å‹: {res.get('è½¦å‹åç§°', 'N/A')}\n"
                context_for_llm += f"  - åŠ¨åŠ›: {res.get('åŠ¨åŠ›ç±»å‹', 'N/A')}\n"
                context_for_llm += f"  - é©±åŠ¨å½¢å¼: {res.get('åº•ç›˜è½¬å‘_é©±åŠ¨å½¢å¼', 'N/A')}\n"
                context_for_llm += f"  - ä»·æ ¼(ä¸‡): {res.get('price_numeric', 'N/A')}\n"
                context_for_llm += "  - å…³é”®é…ç½®:\n"
                if not requested_fields_set:
                    context_for_llm += "    - (æœªæŒ‡å®šç‰¹å®šé…ç½®)\n"
                else:
                    found_config = False
                    cleaned_requested_fields = {f.replace(".keyword", "") for f in requested_fields_set}
                    cleaned_requested_fields.add("è½¦å‹åç§°")
                    cleaned_requested_fields.add("è½¦ç³»åç§°")
                    displayed_configs = set()
                    for field_name in sorted(list(cleaned_requested_fields)):
                        field_value = res.get(field_name, "â€”")
                        display_field_name = re.sub(
                            r"^(?:åŸºæœ¬ä¿¡æ¯|è½¦èº«|åŠ¨åŠ›ç³»ç»Ÿ|å‘åŠ¨æœº|ç”µæœº|ç”µæ± /è¡¥èƒ½|å˜é€Ÿç®±|åº•ç›˜è½¬å‘|è½¦è½®åˆ¶åŠ¨|ä¸»åŠ¨å®‰å…¨|è¾…åŠ©/æ“æ§é…ç½®|å¤–éƒ¨é…ç½®|å†…éƒ¨é…ç½®|åº§æ¤…é…ç½®|å¤šåª’ä½“é…ç½®|æ™ºèƒ½äº’è”|ç¯å…‰é…ç½®|ç»ç’ƒ/åè§†é•œ|ç©ºè°ƒ/å†°ç®±)_",
                            "", field_name)
                        if display_field_name not in displayed_configs:
                            context_for_llm += f"    - {display_field_name}: {field_value}\n"
                            displayed_configs.add(display_field_name)
                            if field_value not in ["â€”", None, ""]:
                                found_config = True
                    if not found_config:
                        context_for_llm += "    - (æœªæ‰¾åˆ°æ‚¨å…³æ³¨çš„é…ç½®ä¿¡æ¯)\n"
                context_for_llm += "\n"
                car_info_for_rec = {
                    "type": "model",
                    "name": res.get('è½¦å‹åç§°', 'N/A'),
                    "series_name": res.get('è½¦ç³»åç§°', 'N/A'),
                    "price": res.get('åŸºæœ¬ä¿¡æ¯_å‚å•†æŒ‡å¯¼ä»·', 'N/A')
                }
                if car_info_for_rec["name"] != 'N/A' and car_info_for_rec["series_name"] != 'N/A':
                    raw_recommended_cars_map[model_id_for_llm] = car_info_for_rec
        else:
            context_for_llm += "åœ¨æˆ‘çš„è½¦è¾†å‚æ•°åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ä¸æ‚¨éœ€æ±‚åŒ¹é…çš„è½¦å‹ã€‚\n\n"
        context_for_llm += "--- ç”¨æˆ·çœŸå®å£ç¢‘èµ„æ–™åº“ (ä¾›ä½ åˆ†æçš„è¯„ä»·) ---\n"
        if review_results:
            for i, result in enumerate(review_results):
                series_name_review = result.get('è½¦ç³»åç§°', 'æœªçŸ¥è½¦ç³»')
                review_text = result.get('æ‰€æœ‰è¯„ä»·', 'æš‚æ— å…·ä½“è¯„ä»·')
                user_model_match = re.search(r"\[ç”¨æˆ·å¡«å†™è½¦å‹:\s*(.*?)\]", review_text)
                user_model_review = user_model_match.group(1).strip() if user_model_match else 'æœªçŸ¥å…·ä½“è½¦å‹'
                review_content = re.sub(r"\[ç”¨æˆ·å¡«å†™è½¦å‹:.*?\]", "", review_text).strip()
                context_for_llm += f"ã€å£ç¢‘èµ„æ–™{i + 1}: {series_name_review} (è½¦ä¸»è½¦å‹: {user_model_review})ã€‘\n"
                context_for_llm += f"  - ç»¼åˆè¯„åˆ†: {result.get('å¹³å‡è¯„åˆ†', 'N/A')}\n"
                context_for_llm += f"  - ç”¨æˆ·è¯„ä»·è¯¦æƒ…: \"{review_content}\"\n\n"
        else:
            context_for_llm += "æ²¡æœ‰æ‰¾åˆ°ä¸æ‚¨éœ€æ±‚ç›´æ¥ç›¸å…³çš„çœŸå®ç”¨æˆ·å£ç¢‘ã€‚\n\n"
        print(f"7. æœ€ç»ˆå‘é€ç»™AIçš„ä¸Šä¸‹æ–‡ (èŠ‚é€‰):\n{context_for_llm[:1000]}...")
        history_for_llm = []
        for msg in chat_history[:-1]:
            if msg.get('role') in ('user', 'assistant'):
                if not (msg.get('role') == 'assistant' and "è¯·å‘Šè¯‰æˆ‘æ‚¨çš„è´­è½¦éœ€æ±‚" in msg.get('content', '')):
                    history_for_llm.append(msg)
        final_query_for_llm = parsed_params.get("keywords_for_llm", current_prompt)
        system_prompt = f"""
ä½ æ˜¯ä¸€ä½èµ„æ·±ã€ä¸“ä¸šä¸”é£è¶£çš„æ±½è½¦æ¨èå®˜æˆ–å¯¹æ¯”åˆ†æå¸ˆã€‚
ä½ çš„ä»»åŠ¡æ˜¯ä¸¥æ ¼æ ¹æ®æˆ‘æä¾›çš„ã€è½¦è¾†å‚æ•°èµ„æ–™åº“ã€‘å’Œã€ç”¨æˆ·çœŸå®å£ç¢‘èµ„æ–™åº“ã€‘æ¥å›ç­”ç”¨æˆ·çš„æœ€æ–°æé—®ã€‚

**ã€ã€ã€è¾“å‡ºæ ¼å¼è¦æ±‚ã€‘ã€‘ã€‘**
ä½ çš„å›ç­”**å¿…é¡»**æ˜¯ä¸€ä¸ª**å•ç‹¬çš„JSONå¯¹è±¡**ï¼ŒåŒ…å«ä»¥ä¸‹ä¸¤ä¸ªé”®ï¼š
1.  `"response_text"` (string): ä½ ç»™ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€å›å¤ã€‚
2.  `"recommended_ids"` (list[string]): ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ï¼ŒåŒ…å«ä½ åœ¨ `response_text` ä¸­æåˆ°æˆ–æ¨èçš„**æ‰€æœ‰**è½¦å‹çš„ã€IDã€‘ã€‚

**ç¤ºä¾‹è¾“å‡ºï¼š**
{{
  "response_text": "æ‚¨å¥½ï¼æ ¹æ®æ‚¨çš„éœ€æ±‚ï¼Œæˆ‘æ¨èã€ID: 1ã€‘å¥¥è¿ªA6Lã€‚ä¸€ä½ã€è½¦ä¸»è½¦å‹: 2024æ¬¾ A6Lã€‘çš„è½¦ä¸»æåˆ°å®ƒ...ã€‚å¤‡é€‰æ–¹æ¡ˆå¯ä»¥è€ƒè™‘ã€ID: 3ã€‘å®é©¬5ç³»ã€‚",
  "recommended_ids": ["1", "3"]
}}

**ã€ã€ã€å›å¤è§„åˆ™ (å¿…é¡»ä¸¥æ ¼éµå®ˆ)ã€‘ã€‘ã€‘**
1.  **IDå¼•ç”¨ (å†…éƒ¨)**: å½“ä½ åœ¨ `response_text` ä¸­æåˆ°èµ„æ–™åº“ä¸­çš„ä»»ä½•è½¦å‹æ—¶ï¼Œ**å¿…é¡»**ä½¿ç”¨ `ã€ID: Xã€‘` çš„æ ¼å¼æ¥å¼•ç”¨å®ƒ (Xæ˜¯èµ„æ–™åº“ä¸­çš„ID)ã€‚
2.  **IDå¡«å…… (å†…éƒ¨)**: å‡¡æ˜¯åœ¨ `response_text` ä¸­è¢« `ã€ID: Xã€‘` å¼•ç”¨çš„è½¦å‹ï¼Œå…¶ ID (ä¾‹å¦‚ "1", "3") **å¿…é¡»**è¢«æ”¶é›†åˆ° `recommended_ids` åˆ—è¡¨ä¸­ã€‚

3.  **ã€ã€ã€æ–°ï¼šæ¨èç»“æ„ã€‘ã€‘ã€‘**:
    * å¦‚æœæ˜¯**æ¨è**ä»»åŠ¡ï¼ˆéå¯¹æ¯”ï¼‰ï¼Œè¯·**å¿…é¡»**ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ç»“æ„å›å¤ï¼š
        * **(1) é¦–é€‰æ¨è (1-2æ¬¾)**: æŒ‘é€‰ 1-2 æ¬¾æœ€åŒ¹é…çš„è½¦å‹ (ã€ID: Xã€‘)ï¼Œä½¿ç”¨ **"ğŸ† é¦–é€‰æ¨èï¼š"** æˆ– **"ğŸ¥ˆ æ¬¡é€‰æ¨èï¼š"** è¿™æ ·çš„æ ‡é¢˜ï¼Œå¹¶è¯¦ç»†åˆ†æã€‚
        * **(2) å¤‡é€‰æ–¹æ¡ˆ (æœ€å¤š 3-4æ¬¾)**: æŒ‘é€‰ 3-4 æ¬¾å…¶ä»–ç¬¦åˆæ¡ä»¶çš„è½¦å‹ (ã€ID: Yã€‘)ï¼Œä½¿ç”¨ **"âœ… å¤‡é€‰æ–¹æ¡ˆï¼š"** æ ‡é¢˜ï¼Œå¹¶ç®€è¦è¯´æ˜ã€‚
        * æ€»æ¨èæ•°**ä¸è¦è¶…è¿‡6æ¬¾**ã€‚
    * å¦‚æœæ˜¯**å¯¹æ¯”**ä»»åŠ¡ï¼Œè¯·åªå¯¹æ¯”ç”¨æˆ·æ˜ç¡®æåˆ°çš„è½¦å‹ï¼Œæ— éœ€ä½¿ç”¨ä¸Šè¿°ç»“æ„ã€‚

4.  **ã€ã€ã€æ–°ï¼šå£ç¢‘å¼•ç”¨ã€‘ã€‘ã€‘**:
    * å½“ä½ å¼•ç”¨ã€ç”¨æˆ·çœŸå®å£ç¢‘èµ„æ–™åº“ã€‘æ—¶ï¼Œ**å¿…é¡»**åœ¨ `response_text` ä¸­æ˜ç¡®æåŠè¯¥å£ç¢‘æ¥è‡ªå“ªä¸ª**ã€è½¦ä¸»è½¦å‹ã€‘** (è¯¥ä¿¡æ¯åœ¨ `ã€å£ç¢‘èµ„æ–™X: ... (è½¦ä¸»è½¦å‹: Y)ã€‘` ä¸­æä¾›)ã€‚
    * *ç¤ºä¾‹*: "ä¸€ä½ã€è½¦ä¸»è½¦å‹: 2023æ¬¾ 530Liã€‘çš„è½¦ä¸»æåˆ°..." æˆ– "æ ¹æ®ã€è½¦ä¸»è½¦å‹: é—®ç•ŒM7 æ™ºé©¾ç‰ˆã€‘çš„å£ç¢‘..."
    * **å¿…é¡»**ç»“åˆå£ç¢‘åˆ†æä¼˜ç¼ºç‚¹ã€‚

5.  **ç¦æ­¢æœæ’°**: ä¸¥ç¦æåŠä»»ä½•ã€è½¦è¾†å‚æ•°èµ„æ–™åº“ã€‘ä¸­æœªåŒ…å«çš„è½¦è¾†ä¿¡æ¯ï¼ˆé™¤éæ˜¯æ ¹æ®è§„åˆ™6è¿›è¡Œè§£é‡Šï¼‰ã€‚
6.  **èµ„æ–™åº“ä¸ºç©º/ä¸å®Œå…¨åŒ¹é…**:
    * å¦‚æœã€è½¦è¾†å‚æ•°èµ„æ–™Kã€‘ä¸ºç©ºï¼Œè¯·åœ¨ `response_text` ä¸­è¯šæ³å‘ŠçŸ¥ç”¨æˆ·æ‰¾ä¸åˆ°æ»¡è¶³*æ‰€æœ‰*æ¡ä»¶çš„è½¦å‹ï¼Œæ­¤æ—¶ `recommended_ids` å¿…é¡»ä¸ºç©ºåˆ—è¡¨ `[]`ã€‚
    * å¦‚æœå¬å›çš„è½¦å‹**éƒ¨åˆ†æ»¡è¶³**æ¡ä»¶ï¼Œè¯·åœ¨ `response_text` ä¸­**æ˜ç¡®æŒ‡å‡º**ã€‚
7.  **ç‰¹æ€§æŸ¥è¯¢ (æ ‡é…/é€‰é…)**: å¦‚æœæ˜¯æŸ¥è¯¢ç‰¹å®šç‰¹æ€§ï¼Œå¿…é¡»ä»”ç»†æ£€æŸ¥ã€å…³é”®é…ç½®ã€‘çš„æ–‡æœ¬å†…å®¹ï¼Œåˆ¤æ–­æ˜¯â€œæ ‡é…â€è¿˜æ˜¯â€œé€‰é…â€å¹¶æ˜ç¡®è¯´æ˜ã€‚
"""
        user_prompt_parts = []
        if has_known_model and has_known_series:
            user_prompt_parts.append(
                f"--- ç”¨æˆ·å½“å‰å…³æ³¨è½¦å‹ ---\nè½¦ç³»: {known_series}\nè½¦å‹: {known_model}\n(è¯·åœ¨åˆ†ææ—¶ä¼˜å…ˆå…³æ³¨èµ„æ–™åº“ä¸­æ ‡è®°ä¸ºã€ç”¨æˆ·å½“å‰å…³æ³¨è½¦å‹èµ„æ–™ã€‘çš„è½¦è¾†)\n---------------------")
        elif has_known_series:
            user_prompt_parts.append(
                f"--- ç”¨æˆ·å½“å‰å…³æ³¨è½¦ç³» ---\nè½¦ç³»: {known_series}\n(è¯·åœ¨åˆ†ææ—¶ä¼˜å…ˆå…³æ³¨è¯¥è½¦ç³»)\n---------------------")
        user_prompt_parts.append(
            f"--- èŠå¤©è®°å½•ä¸Šä¸‹æ–‡ ---\n{json.dumps(history_for_llm, ensure_ascii=False)}\n---------------------")
        user_prompt_parts.append(f"--- èµ„æ–™åº“ (ä½ çš„å”¯ä¸€ä¿¡æ¯æ¥æº) ---\n{context_for_llm}\n---------------------")
        user_prompt_parts.append(f"--- ç”¨æˆ·çš„æœ€æ–°æé—® ---\n\"{final_query_for_llm}\"")
        user_prompt_parts.append("\nè¯·ä¸¥æ ¼æŒ‰ç…§ç³»ç»ŸæŒ‡ä»¤çš„JSONæ ¼å¼è¿”å›ä½ çš„åˆ†æã€‚")
        final_user_prompt = "\n".join(user_prompt_parts)
        messages_to_send = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": final_user_prompt}
        ]
        llm_response = llm_client.chat.completions.create(
            model=DEEPSEEK_MODEL_NAME,
            messages=messages_to_send,
            temperature=0.3,
            response_format={"type": "json_object"}
        )
        ai_response_content = llm_response.choices[0].message.content if llm_response.choices else "{}"
        print(f"8. AIå·²æˆåŠŸè¿”å› (åŸå§‹JSON): {ai_response_content[:300]}...")
        try:
            clean_json_str = re.sub(r"^\s*```json\s*|\s*```\s*$", "", ai_response_content, flags=re.DOTALL).strip()
            ai_data = json.loads(clean_json_str)
            ai_response_text_raw = ai_data.get("response_text", "æŠ±æ­‰ï¼ŒAIæ¨èæœåŠ¡æš‚æ—¶é‡åˆ°é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚")
            ai_recommended_ids = ai_data.get("recommended_ids", [])

            ai_response_text = re.sub(r"ã€ID:\s*\d+ã€‘", "", ai_response_text_raw).strip()
            ai_response_text = re.sub(r"\s*ã€ID:\s*\d+ã€‘\s*", " ", ai_response_text).strip()
        except json.JSONDecodeError as e:
            print(f"   -> (ä¸¥é‡é”™è¯¯: AIæœªè¿”å›æ ‡å‡†JSON: {e})")
            print(f"   -> (åŸå§‹å›å¤): {ai_response_content}")
            ai_response_text = ai_response_content if ai_response_content.strip() else "æŠ±æ­‰ï¼ŒAIæœªèƒ½æ­£ç¡®å¤„ç†æ‚¨çš„è¯·æ±‚ã€‚"
            ai_recommended_ids = []
        final_recommended_cars_list = []
        if ai_recommended_ids:
            print(f"   -> (ç²¾ç¡®è¿‡æ»¤ï¼šAI æ¨èäº† {len(ai_recommended_ids)} ä¸ªID: {ai_recommended_ids})")
            for car_id in ai_recommended_ids:
                car_data = raw_recommended_cars_map.get(str(car_id))
                if car_data:
                    final_recommended_cars_list.append(car_data)
                else:
                    print(f"   -> (è­¦å‘Š: AI æ¨èäº†ä¸å­˜åœ¨çš„ ID: {car_id})")
        else:
            print(f"   -> (ç²¾ç¡®è¿‡æ»¤ï¼šAI æœªæ¨èä»»ä½•è½¦å‹ID)")
        return jsonify({
            "response_text": ai_response_text,
            "recommended_cars": final_recommended_cars_list
        })
    except Exception as e:
        print(f"--- [AI CHAT ERROR] ---")
        traceback.print_exc()
        return jsonify({"error": f"AIèŠå¤©æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {e}"}), 500


@app.route('/get-review-summary', methods=['GET'])
def get_review_summary():
    if not llm_client: return jsonify({"error": "AIæœåŠ¡æœªé…ç½®æˆ–ä¸å¯ç”¨"}), 503
    if not es_client: return jsonify({"error": "æ•°æ®åº“æœåŠ¡æœªè¿æ¥"}), 503

    series_name = request.args.get('series_name', type=str)
    if not series_name:
        return jsonify({"error": "å¿…é¡»æä¾›è½¦ç³»åç§°"}), 400

    filters_str = request.args.get('filters', '{}')
    filter_context = ""
    try:
        filters = json.loads(filters_str)
        filter_parts = []
        if filters.get('power_type') and filters['power_type'] != 'ä¸é™': filter_parts.append(filters['power_type'])
        if filters.get('body_type') and filters['body_type'] != 'ä¸é™': filter_parts.append(filters['body_type'])
        if filters.get('segment') and filters['segment'] != 'ä¸é™': filter_parts.append(filters['segment'])
        price_min = filters.get('price_min', 0)
        price_max = filters.get('price_max', 0)
        if price_min > 0 and price_max > 0:
            filter_parts.append(f"{price_min}-{price_max}ä¸‡")
        elif price_min > 0:
            filter_parts.append(f"{price_min}ä¸‡ä»¥ä¸Š")
        elif price_max > 0:
            filter_parts.append(f"{price_max}ä¸‡ä»¥å†…")
        filter_context = ", ".join(filter_parts)
    except Exception as e:
        print(f"è§£æAIæ€»ç»“è¿‡æ»¤å™¨å¤±è´¥: {e}")

    print(f"\n--- [AI SUMMARY DEBUG] ---")
    print(f"1. å¼€å§‹ä¸º {series_name} ç”Ÿæˆå£ç¢‘æ€»ç»“...")
    print(f"   -> è¿‡æ»¤å™¨ä¸Šä¸‹æ–‡: {filter_context}")
    try:
        review_query = {
            "query": {
                "bool": {"must": [{"term": {"is_koubei_row": True}}, {"term": {"è½¦ç³»åç§°.keyword": series_name}}]}},
            "size": 1
        }
        review_response = es_client.search(index=INDEX_NAME, body=review_query)
        review_results_raw = [hit['_source'] for hit in review_response['hits']['hits']]

        if not review_results_raw:
            print(f"2. æœªæ‰¾åˆ° {series_name} çš„å£ç¢‘æ•°æ®ã€‚")
            return jsonify({"error": "æš‚æ— è¯¥è½¦ç³»çš„ç”¨æˆ·å£ç¢‘æ•°æ®ã€‚"}), 404

        review_doc = clean_es_result(review_results_raw[0])
        average_rating = review_doc.get('å¹³å‡è¯„åˆ†')
        review_count = review_doc.get('è¯„ä»·æ•°é‡')
        reviews_context = review_doc.get('æ‰€æœ‰è¯„ä»·')

        if not reviews_context:
            print(f"2. {series_name} å£ç¢‘æ•°æ®ä¸­ç¼ºå°‘[æ‰€æœ‰è¯„ä»·]å­—æ®µã€‚")
            return jsonify({"error": "å£ç¢‘æ•°æ®ä¸å®Œæ•´ï¼Œç¼ºå°‘è¯„ä»·å†…å®¹ã€‚"}), 404

        print(f"2. å·²æ±‡æ€» {series_name} çš„å£ç¢‘æ•°æ®ã€‚è¯„åˆ†: {average_rating}, æ•°é‡: {review_count}")

        summary_prompt = create_summary_prompt(series_name, reviews_context, filter_context)
        llm_response = llm_client.chat.completions.create(
            model=DEEPSEEK_MODEL_NAME,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ±½è½¦ç¼–è¾‘ï¼Œä½ çš„ä»»åŠ¡æ˜¯æ€»ç»“ç”¨æˆ·å£ç¢‘ã€‚"},
                {"role": "user", "content": summary_prompt}
            ],
            temperature=0.2
        )
        ai_summary = llm_response.choices[0].message.content if llm_response.choices else "AIæ€»ç»“ç”Ÿæˆå¤±è´¥ï¼Œè¯·ç¨åå†è¯•ã€‚"
        print(f"3. AIæ€»ç»“å·²ç”Ÿæˆã€‚")

        return jsonify({
            "average_rating": average_rating,
            "review_count": review_count,
            "summary_text": ai_summary
        })

    except Exception as e:
        print(f"--- [AI SUMMARY ERROR] ---")
        traceback.print_exc()
        return jsonify({"error": f"ç”ŸæˆAIæ€»ç»“æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {e}"}), 500


def create_summary_prompt(series_name, reviews_context, filter_context):
    context_prompt = f"è¯·ç‰¹åˆ«å…³æ³¨ç”¨æˆ·æ­£åœ¨ç­›é€‰çš„æ¡ä»¶ï¼šã€{filter_context}ã€‘ï¼Œå¹¶åœ¨æ€»ç»“æ—¶ä¼˜å…ˆä½“ç°ä¸è¿™äº›æ¡ä»¶ç›¸å…³çš„ä¼˜ç¼ºç‚¹ã€‚" if filter_context else ""
    return f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ±½è½¦ç¼–è¾‘ï¼Œä½ çš„ä»»åŠ¡æ˜¯åˆ†æç»™å®šè½¦ç³»çš„çœŸå®ç”¨æˆ·å£ç¢‘ï¼Œå¹¶ç”¨å®¢è§‚ã€ç²¾ç‚¼çš„è¯­è¨€æ€»ç»“å‡ºä¼˜ç‚¹å’Œç¼ºç‚¹ã€‚
**ä»»åŠ¡:**
æ ¹æ®ä»¥ä¸‹å…³äºã€{series_name}ã€‘è½¦ç³»çš„çœŸå®ç”¨æˆ·å£ç¢‘ï¼Œæ€»ç»“å‡ºç”¨æˆ·æœ€å¸¸æåŠçš„ 3 ä¸ªä¸»è¦ä¼˜ç‚¹å’Œ 3 ä¸ªä¸»è¦ç¼ºç‚¹ã€‚
{context_prompt}
**è§„åˆ™:**
1.  **å®¢è§‚å…¬æ­£**: ä¸¥æ ¼åŸºäºæä¾›çš„å£ç¢‘åŸæ–‡ï¼Œä¸è¦æœæ’°ã€‚
2.  **é«˜åº¦å‡ç»ƒ**: æ¯ä¸ªä¼˜ç‚¹å’Œç¼ºç‚¹è¯·ç”¨ä¸€å¥è¯æ¦‚æ‹¬ã€‚
3.  **å¼•ç”¨ä½è¯**: åœ¨æ¯ä¸€æ¡æ€»ç»“åé¢ï¼Œå¿…é¡»ç›´æ¥å¼•ç”¨ 1-2 å¥æœ€ç›¸å…³çš„ç”¨æˆ·åŸè¯ä½œä¸ºè¯æ®ã€‚
4.  **ä¸“æ³¨é«˜é¢‘**: åªæ€»ç»“è¢«å¤šäººåå¤æåŠçš„æ ¸å¿ƒè§‚ç‚¹ã€‚
5.  **æ ¼å¼è¦æ±‚**: å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼Œä¸è¦æœ‰ä»»ä½•å¤šä½™çš„è§£é‡Šï¼š
**ã€{series_name} - AIå£ç¢‘æ€»ç»“ã€‘**
**ä¸»è¦ä¼˜ç‚¹:**
1.  **[ä¼˜ç‚¹1]**: [æ€»ç»“çš„ä¼˜ç‚¹ä¸€å¥è¯]
    * *ç”¨æˆ·è¯„ä»·ä½è¯:* "[å¼•ç”¨çš„ç”¨æˆ·åŸè¯...]"
2.  **[ä¼˜ç‚¹2]**: [æ€»ç»“çš„ä¼˜ç‚¹ä¸€å¥è¯]
    * *ç”¨æˆ·è¯„ä»·ä½è¯:* "[å¼•ç”¨çš„ç”¨æˆ·åŸè¯...]"
3.  **[ä¼˜ç‚¹3]**: [æ€»ç»“çš„ä¼˜ç‚¹ä¸€å¥è¯]
    * *ç”¨æˆ·è¯„ä»·ä½è¯:* "[å¼•ç”¨çš„ç”¨æˆ·åŸè¯...]"
**ä¸»è¦ç¼ºç‚¹:**
1.  **[ç¼ºç‚¹1]**: [æ€»ç»“çš„ç¼ºç‚¹ä¸€å¥è¯]
    * *ç”¨æˆ·è¯„ä»·ä½è¯:* "[å¼•ç”¨çš„ç”¨æˆ·åŸè¯...]"
2.  **[ç¼ºç‚¹2]**: [æ€»ç»“çš„ç¼ºç‚¹ä¸€å¥è¯]
    * *ç”¨æˆ·è¯„ä»·ä½è¯:* "[å¼•ç”¨çš„ç”¨æˆ·åŸè¯...]"
3.  **[ç¼ºç‚¹3]**: [æ€»ç»“çš„ç¼ºç‚¹ä¸€å¥è¯]
    * *ç”¨æˆ·è¯„ä»·ä½è¯:* "[å¼•ç”¨çš„ç”¨æˆ·åŸè¯...]"
---
**ã€ç”¨æˆ·å£ç¢‘åŸæ–‡å¦‚ä¸‹ã€‘:**
{reviews_context}
"""


@app.route('/ai_compare', methods=['POST'])
def ai_compare():
    if not llm_client:
        return jsonify({"error": "AIæœåŠ¡æœªé…ç½®æˆ–ä¸å¯ç”¨"}), 503
    try:
        models_data = request.json
        if not models_data or not isinstance(models_data, list) or len(models_data) < 2:
            return jsonify({"error": "å¿…é¡»æä¾›è‡³å°‘ä¸¤ä¸ªè½¦å‹è¿›è¡Œå¯¹æ¯”"}), 400

        print(f"\n--- [AI COMPARE DEBUG] ---")
        print(f"1. æ”¶åˆ° {len(models_data)} ä¸ªè½¦å‹çš„å¯¹æ¯”è¯·æ±‚ã€‚")

        key_fields_map = {
            "è½¦å‹åç§°": "è½¦å‹åç§°",
            "ä»·æ ¼": "åŸºæœ¬ä¿¡æ¯_å‚å•†æŒ‡å¯¼ä»·",
            "åŠ¨åŠ›ç±»å‹": "åŠ¨åŠ›ç±»å‹",
            "å‘åŠ¨æœºæ’é‡(L)": "å‘åŠ¨æœº_æ’é‡[L]",
            "å‘åŠ¨æœºæœ€å¤§åŠŸç‡(kW)": "å‘åŠ¨æœº_æœ€å¤§åŠŸç‡[kW]",
            "ç”µæœºæ€»åŠŸç‡(kW)": "ç”µæœº_æ€»åŠŸç‡[kW]",
            "CLTCçº¯ç”µç»­èˆª(km)": "ç”µæ± /è¡¥èƒ½_CLTCçº¯ç”µç»­èˆªé‡Œç¨‹[km]",
            "é•¿*å®½*é«˜(mm)": "è½¦èº«_é•¿*å®½*é«˜[mm]",
            "è½´è·(mm)": "è½¦èº«_è½´è·[mm]",
            "åº§ä½æ•°": "è½¦èº«_åº§ä½æ•°",
            "è¾…åŠ©é©¾é©¶çº§åˆ«": "è¾…åŠ©é©¾é©¶ç¡¬ä»¶_é©¾é©¶è¾…åŠ©çº§åˆ«",
            "å‰æ’åº§æ¤…åŠŸèƒ½": "åº§æ¤…é…ç½®_ç¬¬ä¸€æ’åº§æ¤…åŠŸèƒ½",
            "ä¸­æ§å±å°ºå¯¸": "è½¦æœº/äº’è”_ä¸­æ§å±å°ºå¯¸[è‹±å¯¸]",
            "é©±åŠ¨å½¢å¼": "åº•ç›˜è½¬å‘_é©±åŠ¨å½¢å¼"
        }

        user_prompt_text = "è¯·å¸®æˆ‘è¯¦ç»†å¯¹æ¯”ä»¥ä¸‹å‡ æ¬¾è½¦ï¼š\n\n"
        car_count = 1
        for model in models_data:
            model_name = model.get("è½¦å‹åç§°", f"æœªçŸ¥è½¦å‹ {car_count}")
            user_prompt_text += f"--- è½¦å‹ {car_count}: ã€{model_name}ã€‘ ---\n"

            for display_name, internal_key in key_fields_map.items():
                value = model.get(internal_key, "â€”")

                if value and value != "â€”" and value != "N/A" and value is not None:
                    user_prompt_text += f"- {display_name}: {value}\n"

            user_prompt_text += "\n"
            car_count += 1

        user_prompt_text += "--- å¯¹æ¯”è¦æ±‚ ---\n"
        user_prompt_text += "è¯·ä»ã€ä»·æ ¼ä¸æ€§ä»·æ¯”ã€‘ã€ã€åŠ¨åŠ›ä¸æ“æ§ã€‘ã€ã€ç©ºé—´ä¸èˆ’é€‚æ€§ã€‘å’Œã€æ™ºèƒ½åŒ–é…ç½®ã€‘è¿™å‡ ä¸ªæ ¸å¿ƒè§’åº¦ï¼Œç”¨ä¸­æ–‡è¯¦ç»†åˆ†æå®ƒä»¬å„è‡ªçš„ã€ä¸»è¦ä¼˜ç‚¹ã€‘å’Œã€ä¸»è¦ç¼ºç‚¹ã€‘ã€‚\n"
        user_prompt_text += "æœ€åï¼Œè¯·æ€»ç»“ä¸€ä¸‹å®ƒä»¬åˆ†åˆ«ã€é€‚åˆä»€ä¹ˆæ ·çš„äººç¾¤ã€‘ï¼Œå¹¶ç»™æˆ‘ä¸€ä¸ªæœ€ç»ˆçš„è´­ä¹°å»ºè®®ã€‚"

        system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€èµ„æ·±ã€å®¢è§‚çš„æ±½è½¦å¯¹æ¯”è¯„æµ‹ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä¸¥æ ¼æ ¹æ®ç”¨æˆ·æä¾›çš„å‡ æ¬¾è½¦å‹çš„æ ¸å¿ƒå‚æ•°ï¼Œç”Ÿæˆä¸€ä»½è¯¦ç»†çš„å¯¹æ¯”æŠ¥å‘Šã€‚è¯·ä½¿ç”¨æ¸…æ™°ã€ä¸“ä¸šçš„è¯­è¨€ï¼Œç›´æ¥å›ç­”ç”¨æˆ·çš„å¯¹æ¯”è¦æ±‚ï¼Œä¸è¦è¯´å¤šä½™çš„å®¢å¥—è¯ã€‚"

        print(f"2. æ­£åœ¨å‘ AI å‘é€å¯¹æ¯” Prompt...")

        messages_to_send = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt_text}
        ]

        llm_response = llm_client.chat.completions.create(
            model=DEEPSEEK_MODEL_NAME,
            messages=messages_to_send,
            temperature=0.2
        )

        ai_summary = llm_response.choices[0].message.content if llm_response.choices else "AIæ€»ç»“ç”Ÿæˆå¤±è´¥ï¼Œè¯·ç¨åå†è¯•ã€‚"
        print(f"3. AIå¯¹æ¯”æ€»ç»“å·²ç”Ÿæˆã€‚")

        return jsonify({"summary": ai_summary})

    except Exception as e:
        print(f"--- [AI COMPARE ERROR] ---")
        traceback.print_exc()
        return jsonify({"error": f"ç”ŸæˆAIå¯¹æ¯”æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {e}"}), 500


@app.route('/api/admin/users', methods=['GET'])
@admin_required()
def admin_get_users():
    try:
        requesting_user = get_current_user_from_jwt()
        users_query = User.query.order_by(User.role.desc(), User.id)

        grouped_users = {
            "core_admin": [],
            "admin": [],
            "user": []
        }

        # core_admin å¯ä»¥çœ‹åˆ°æ‰€æœ‰äºº
        if requesting_user.role == 'core_admin':
            all_users = users_query.all()
            for u in all_users:
                user_data = {"id": u.id, "username": u.username, "nickname": u.nickname, "role": u.role,
                             "is_banned": u.is_banned, "ban_reason": u.ban_reason}
                if u.role == 'core_admin':
                    grouped_users["core_admin"].append(user_data)
                elif u.role == 'admin':
                    grouped_users["admin"].append(user_data)
                else:
                    grouped_users["user"].append(user_data)

        # admin åªèƒ½çœ‹åˆ° user
        elif requesting_user.role == 'admin':
            # æŸ¥è¯¢ 'admin' å’Œ 'user' ä¸¤ç§è§’è‰², ä½†æ’é™¤ 'core_admin'
            users_and_admins = users_query.filter(User.role.in_(['admin', 'user'])).all()

            for u in users_and_admins:
                user_data = {"id": u.id, "username": u.username, "nickname": u.nickname, "role": u.role,
                             "is_banned": u.is_banned, "ban_reason": u.ban_reason}

                if u.role == 'admin':
                    grouped_users["admin"].append(user_data)
                elif u.role == 'user':
                    grouped_users["user"].append(user_data)

            # éšè— core_admin åˆ—è¡¨
            del grouped_users["core_admin"]

        return jsonify(grouped_users), 200

    except Exception as e:
        traceback.print_exc()
        return jsonify({"error": f"è·å–ç”¨æˆ·åˆ—è¡¨æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {str(e)}"}), 500


@app.route('/api/admin/users/update_role', methods=['POST'])
@admin_required()
def admin_update_user_role():
    try:
        requesting_user = get_current_user_from_jwt()
        data = request.json
        user_id = data.get('user_id')
        new_role = data.get('new_role')

        if not user_id or not new_role:
            return jsonify({"error": "å¿…é¡»æä¾› user_id å’Œ new_role"}), 400

        if new_role not in ['user', 'admin']:
            return jsonify({"error": "æ— æ•ˆçš„è§’è‰²ç›®æ ‡"}), 400

        user_to_update = User.query.get(int(user_id))
        if not user_to_update:
            return jsonify({"error": "ç›®æ ‡ç”¨æˆ·ä¸å­˜åœ¨"}), 404

        if user_to_update.role == 'core_admin':
            return jsonify({"error": "æƒé™ä¸è¶³ï¼šæ— æ³•ä¿®æ”¹æ ¸å¿ƒç®¡ç†å‘˜"}), 403

        if requesting_user.role == 'admin':
            if user_to_update.role != 'user':
                return jsonify({"error": "æƒé™ä¸è¶³ï¼šç®¡ç†å‘˜åªèƒ½ç®¡ç†æ™®é€šç”¨æˆ·"}), 403
            if new_role != 'user':
                return jsonify({"error": "æƒé™ä¸è¶³ï¼šç®¡ç†å‘˜ä¸èƒ½æå‡ä»–äººæƒé™"}), 403

        user_to_update.role = new_role
        db.session.commit()
        print(f"   -> (ç®¡ç†å‘˜æ“ä½œ {requesting_user.username}ï¼šç”¨æˆ· {user_to_update.username} è§’è‰²å·²æ›´æ–°ä¸º {new_role})")
        return jsonify({"message": "ç”¨æˆ·è§’è‰²æ›´æ–°æˆåŠŸ"}), 200

    except Exception as e:
        db.session.rollback()
        traceback.print_exc()
        return jsonify({"error": f"æ›´æ–°è§’è‰²æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {str(e)}"}), 500


@app.route('/api/admin/users/ban', methods=['POST'])
@admin_required()
def admin_ban_user():
    try:
        requesting_user = get_current_user_from_jwt()
        data = request.json
        user_id = data.get('user_id')
        reason = data.get('reason', 'æ— ç‰¹å®šåŸå› ')

        if not user_id:
            return jsonify({"error": "å¿…é¡»æä¾› user_id"}), 400

        user_to_ban = User.query.get(int(user_id))
        if not user_to_ban:
            return jsonify({"error": "ç›®æ ‡ç”¨æˆ·ä¸å­˜åœ¨"}), 404

        if user_to_ban.role == 'core_admin':
            return jsonify({"error": "æƒé™ä¸è¶³ï¼šæ— æ³•å°ç¦æ ¸å¿ƒç®¡ç†å‘˜"}), 403
        if requesting_user.role == 'admin' and user_to_ban.role == 'admin':
            return jsonify({"error": "æƒé™ä¸è¶³ï¼šç®¡ç†å‘˜æ— æ³•å°ç¦å…¶ä»–ç®¡ç†å‘˜"}), 403

        user_to_ban.is_banned = True
        user_to_ban.ban_reason = reason
        db.session.commit()
        print(f"   -> (ç®¡ç†å‘˜æ“ä½œ {requesting_user.username}ï¼šç”¨æˆ· {user_to_ban.username} å·²è¢«å°ç¦)")
        return jsonify({"message": "ç”¨æˆ·å°ç¦æˆåŠŸ"}), 200

    except Exception as e:
        db.session.rollback()
        return jsonify({"error": f"å°ç¦ç”¨æˆ·æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {str(e)}"}), 500


@app.route('/api/admin/users/unban', methods=['POST'])
@admin_required()
def admin_unban_user():
    try:
        requesting_user = get_current_user_from_jwt()
        data = request.json
        user_id = data.get('user_id')

        if not user_id:
            return jsonify({"error": "å¿…é¡»æä¾› user_id"}), 400

        user_to_unban = User.query.get(int(user_id))
        if not user_to_unban:
            return jsonify({"error": "ç›®æ ‡ç”¨æˆ·ä¸å­˜åœ¨"}), 404

        if user_to_unban.role == 'core_admin':
            return jsonify({"error": "æƒé™ä¸è¶³ï¼šæ— æ³•æ“ä½œæ ¸å¿ƒç®¡ç†å‘˜"}), 403
        if requesting_user.role == 'admin' and user_to_unban.role == 'admin':
            return jsonify({"error": "æƒé™ä¸è¶³ï¼šç®¡ç†å‘˜æ— æ³•æ“ä½œå…¶ä»–ç®¡ç†å‘˜"}), 403

        user_to_unban.is_banned = False
        user_to_unban.ban_reason = None
        db.session.commit()
        print(f"   -> (ç®¡ç†å‘˜æ“ä½œ {requesting_user.username}ï¼šç”¨æˆ· {user_to_unban.username} å·²è¢«è§£å°)")
        return jsonify({"message": "ç”¨æˆ·è§£å°æˆåŠŸ"}), 200

    except Exception as e:
        db.session.rollback()
        return jsonify({"error": f"è§£å°ç”¨æˆ·æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {str(e)}"}), 500


@app.route('/api/admin/users/<int:user_id>', methods=['DELETE'])
@core_admin_required()
def admin_delete_user(user_id):
    try:
        data = request.json
        password = data.get('high_risk_password')

        is_valid, message = _verify_high_risk_password(password)
        if not is_valid:
            return jsonify({"error": message}), 403

        user_to_delete = User.query.get(user_id)
        if not user_to_delete:
            return jsonify({"error": "ç›®æ ‡ç”¨æˆ·ä¸å­˜åœ¨"}), 404

        if user_to_delete.role == 'core_admin':
            return jsonify({"error": "æ— æ³•åˆ é™¤æ ¸å¿ƒç®¡ç†å‘˜"}), 403

        username = user_to_delete.username
        db.session.delete(user_to_delete)
        db.session.commit()
        print(f"   -> (ç®¡ç†å‘˜æ“ä½œï¼šç”¨æˆ· {username} (ID: {user_id}) å·²è¢«åˆ é™¤)")
        return jsonify({"message": "ç”¨æˆ·åˆ é™¤æˆåŠŸ"}), 200

    except Exception as e:
        db.session.rollback()
        traceback.print_exc()
        return jsonify({"error": f"åˆ é™¤ç”¨æˆ·æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {str(e)}"}), 500


@app.route('/api/admin/ai_config', methods=['GET'])
@core_admin_required()
def admin_get_ai_config():
    try:
        if not os.path.exists(MAPPING_FILE_PATH):
            return jsonify({"error": "é…ç½®æ–‡ä»¶ feature_mapping.json æœªæ‰¾åˆ°"}), 404

        with open(MAPPING_FILE_PATH, 'r', encoding='utf-8') as f:
            config_data = json.load(f)

        return jsonify(config_data), 200

    except json.JSONDecodeError:
        return jsonify({"error": "é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„JSON"}), 500
    except Exception as e:
        traceback.print_exc()
        return jsonify({"error": f"è¯»å–é…ç½®æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {str(e)}"}), 500


@app.route('/api/admin/ai_config', methods=['POST'])
@core_admin_required()
def admin_save_ai_config():
    try:
        request_data = request.json
        if not isinstance(request_data, dict):
            return jsonify({"error": "æ— æ•ˆçš„é…ç½®æ ¼å¼"}), 400

        password = request_data.pop('high_risk_password', None)
        new_config_data = request_data

        is_valid, message = _verify_high_risk_password(password)
        if not is_valid:
            return jsonify({"error": message}), 403

        with open(MAPPING_FILE_PATH, 'w', encoding='utf-8') as f:
            json.dump(new_config_data, f, indent=2, ensure_ascii=False)

        global FEATURE_MAPPING
        FEATURE_MAPPING.clear()
        FEATURE_MAPPING.update(new_config_data)

        print(f"   -> (ç®¡ç†å‘˜æ“ä½œï¼šAI ç‰¹æ€§æ˜ å°„å·²æ›´æ–°å¹¶çƒ­åŠ è½½)")
        return jsonify({"message": "AI é…ç½®ä¿å­˜æˆåŠŸï¼Œå¹¶å·²åœ¨æœåŠ¡å™¨ç”Ÿæ•ˆ"}), 200

    except Exception as e:
        traceback.print_exc()
        return jsonify({"error": f"ä¿å­˜é…ç½®æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {str(e)}"}), 500


@app.route('/api/admin/vehicles/search', methods=['GET'])
@admin_required()
def admin_search_vehicles():
    if not es_client:
        return jsonify({"error": "ES æœåŠ¡æœªè¿æ¥"}), 503

    try:
        q = request.args.get('q', '').strip()
        page = request.args.get('page', 1, type=int)
        per_page = 20

        query_body = {
            "from": (page - 1) * per_page,
            "size": per_page,
            "query": {
                "bool": {
                    "must": [],
                    "should": [],
                    "minimum_should_match": 0
                }
            },
            "sort": ["_doc"]
        }

        if not q:
            query_body['query']['bool']['must'].append({"match_all": {}})
        else:
            query_body['query']['bool']['minimum_should_match'] = 1
            query_body['query']['bool']['should'] = [
                {"match_phrase_prefix": {"è½¦å‹åç§°": {"query": q, "boost": 10}}},
                {"match_phrase_prefix": {"è½¦ç³»åç§°": {"query": q, "boost": 5}}},
                {"wildcard": {"è½¦å‹åç§°.keyword": {"value": f"*{q}*", "case_insensitive": True, "boost": 2}}},
                {"wildcard": {"è½¦ç³»åç§°.keyword": {"value": f"*{q}*", "case_insensitive": True, "boost": 2}}}
            ]
            query_body['sort'] = [{"_score": "desc"}]

        response = es_client.search(index=INDEX_NAME, body=query_body)

        hits = response['hits']['hits']
        total = response['hits']['total']['value']

        return jsonify({
            "hits": hits,
            "total": total,
            "page": page,
            "per_page": per_page
        })

    except Exception as e:
        traceback.print_exc()
        return jsonify({"error": f"æœç´¢ ES æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {str(e)}"}), 500


@app.route('/api/admin/vehicles/delete', methods=['POST'])
@admin_required()
def admin_delete_vehicle():
    if not es_client:
        return jsonify({"error": "ES æœåŠ¡æœªè¿æ¥"}), 503

    try:
        data = request.json
        doc_id = data.get('doc_id')

        password = data.get('high_risk_password')
        is_valid, message = _verify_high_risk_password(password)
        if not is_valid:
            return jsonify({"error": message}), 403
        if not doc_id:
            return jsonify({"error": "å¿…é¡»æä¾› doc_id"}), 400

        response = es_client.delete(index=INDEX_NAME, id=doc_id, ignore=[404])

        if response.get('result') == 'deleted':
            print(f"   -> (ç®¡ç†å‘˜æ“ä½œï¼šES æ–‡æ¡£ {doc_id} å·²è¢«åˆ é™¤)")
            return jsonify({"message": "æ–‡æ¡£åˆ é™¤æˆåŠŸ"}), 200
        elif response.get('result') == 'not_found':
            return jsonify({"error": "æ–‡æ¡£æœªæ‰¾åˆ°ï¼Œå¯èƒ½å·²è¢«åˆ é™¤"}), 404
        else:
            return jsonify({"error": "åˆ é™¤å¤±è´¥", "details": response}), 500

    except NotFoundError:
        return jsonify({"error": "æ–‡æ¡£æœªæ‰¾åˆ°"}), 404
    except Exception as e:
        traceback.print_exc()
        return jsonify({"error": f"åˆ é™¤ ES æ–‡æ¡£æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {str(e)}"}), 500


@app.route('/api/admin/vehicles/get/<string:doc_id>', methods=['GET'])
@admin_required()
def admin_get_vehicle_doc(doc_id):
    if not es_client:
        return jsonify({"error": "ES æœåŠ¡æœªè¿æ¥"}), 503
    try:
        response = es_client.get(index=INDEX_NAME, id=doc_id)
        return jsonify(response['_source']), 200

    except NotFoundError:
        return jsonify({"error": "æœªæ‰¾åˆ°è¯¥æ–‡æ¡£ (ID: " + doc_id + ")"}), 404
    except Exception as e:
        traceback.print_exc()
        return jsonify({"error": f"è·å– ES æ–‡æ¡£æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {str(e)}"}), 500


@app.route('/api/admin/vehicles/update/<string:doc_id>', methods=['POST'])
@admin_required()
def admin_update_vehicle_doc(doc_id):
    if not es_client:
        return jsonify({"error": "ES æœåŠ¡æœªè¿æ¥"}), 503
    try:
        form_data = request.json
        if not form_data or not isinstance(form_data, dict):
            return jsonify({"error": "æ— æ•ˆçš„JSONæ•°æ®"}), 400
        if 'high_risk_password' in form_data:
            del form_data['high_risk_password']

        try:
            existing_doc_response = es_client.get(index=INDEX_NAME, id=doc_id)
            doc_data = existing_doc_response['_source']
        except NotFoundError:
            return jsonify({"error": "æœªæ‰¾åˆ°è¯¥æ–‡æ¡£ (ID: " + doc_id + ")ï¼Œæ— æ³•æ›´æ–°"}), 404

        doc_data.update(form_data)

        doc_data['price_numeric'] = parse_price_to_numeric(doc_data.get('åŸºæœ¬ä¿¡æ¯_å‚å•†æŒ‡å¯¼ä»·'))
        doc_data['åŠ¨åŠ›ç±»å‹'] = clean_power_type(doc_data.get('åŠ¨åŠ›ç±»å‹'))
        doc_data['è½¦èº«ç±»å‹'] = clean_body_type(doc_data.get('åŸºæœ¬ä¿¡æ¯_è½¦èº«ç»“æ„'))
        doc_data['è½¦èº«_åº§ä½æ•°'] = clean_seat_count(doc_data.get('åŸºæœ¬ä¿¡æ¯_è½¦èº«ç»“æ„'))
        doc_data['åŸºæœ¬ä¿¡æ¯_çº§åˆ«'] = clean_segment(doc_data.get('åŸºæœ¬ä¿¡æ¯_çº§åˆ«'))
        model_name = doc_data.get('è½¦å‹åç§°', '')
        doc_data['is_koubei_row'] = bool(isinstance(model_name, str) and 'å£ç¢‘' in model_name)

        if 'å›¾ç‰‡é“¾æ¥' not in doc_data or not doc_data['å›¾ç‰‡é“¾æ¥']:
            doc_data['å›¾ç‰‡é“¾æ¥'] = 'https://p1.itc.cn/images01/20240306/633735165b3e4192be167d55f013d5a1.jpeg'

        final_doc = {k: v for k, v in doc_data.items() if v is not None}

        es_client.index(
            index=INDEX_NAME,
            id=doc_id,
            body=final_doc,
            refresh=True
        )

        print(f"   -> (ç®¡ç†å‘˜æ“ä½œï¼šES æ–‡æ¡£ {doc_id} å·²è¢«ã€v2ä¿®å¤å¹¶ã€‘æ›´æ–°)")
        return jsonify({"message": "æ–‡æ¡£æ›´æ–°æˆåŠŸ"}), 200

    except NotFoundError:
        return jsonify({"error": "æœªæ‰¾åˆ°è¯¥æ–‡æ¡£ï¼Œæ— æ³•æ›´æ–°"}), 404
    except Exception as e:
        traceback.print_exc()
        return jsonify({"error": f"æ›´æ–° ES æ–‡æ¡£æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {str(e)}"}), 500


@app.route('/api/admin/system/set_high_risk_password', methods=['POST'])
@core_admin_required()
def admin_set_high_risk_password():
    try:
        data = request.json
        new_password = data.get('password')

        if not new_password or len(new_password) < 6:
            return jsonify({"error": "å¯†ç é•¿åº¦ä¸èƒ½å°‘äº6ä½"}), 400

        hashed_pw = bcrypt.hashpw(new_password.encode('utf-8'), bcrypt.gensalt()).decode('utf-8')

        config = SystemConfig.query.get('high_risk_password')
        if config:
            config.value = hashed_pw
        else:
            config = SystemConfig(key='high_risk_password', value=hashed_pw)
            db.session.add(config)

        db.session.commit()
        print(f"   -> (ç®¡ç†å‘˜æ“ä½œï¼šé«˜å±æ“ä½œå¯†ç å·²è¢«é‡ç½®)")
        return jsonify({"message": "é«˜å±æ“ä½œå¯†ç è®¾ç½®æˆåŠŸ"}), 200

    except Exception as e:
        db.session.rollback()
        traceback.print_exc()
        return jsonify({"error": f"è®¾ç½®å¯†ç æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {str(e)}"}), 500


def _run_subprocess(command):
    try:
        process = subprocess.run(
            command,
            capture_output=True,
            text=True,
            encoding='utf-8',
            errors='replace',
            check=True
        )

        return process.stdout + (f"\n[Debug] {process.stderr}" if process.stderr else ""), None

    except subprocess.CalledProcessError as e:
        error_msg = f"å¯¼å…¥/çˆ¬è™«è„šæœ¬æ‰§è¡Œå¤±è´¥ (Exit Code {e.returncode})ï¼š\n"
        error_msg += e.stdout + "\n" + e.stderr
        return None, error_msg
    except FileNotFoundError as e:
        error_msg = f"å‘½ä»¤æ‰§è¡Œå¤±è´¥ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ã€‚\n{e}"
        return None, error_msg
    except Exception as e:
        error_msg = f"å­è¿›ç¨‹å‘ç”ŸæœªçŸ¥é”™è¯¯ï¼š\n{e}"
        return None, error_msg


@app.route('/api/admin/system/run_crawler', methods=['POST'])
@admin_required()
def admin_run_crawler():
    try:
        urls_to_crawl = request.json.get('urls', [])
        if not urls_to_crawl or not all(isinstance(url, str) for url in urls_to_crawl):
            return jsonify({"error": "å¿…é¡»æä¾›ä¸€ä¸ª URL åˆ—è¡¨"}), 400

        try:
            if os.path.exists(CRAWLER_STATUS_FILE):
                with open(CRAWLER_STATUS_FILE, 'r', encoding='utf-8') as f:
                    status_data = json.load(f)
                    if status_data.get('status') == 'running':
                        return jsonify({"error": "å¯åŠ¨å¤±è´¥ï¼šä¸€ä¸ªçˆ¬è™«ä»»åŠ¡å·²ç»åœ¨åå°è¿è¡Œä¸­ã€‚"}), 409

            if os.path.exists(CRAWLER_LOG_FILE):
                os.remove(CRAWLER_LOG_FILE)

            with open(CRAWLER_STATUS_FILE, 'w', encoding='utf-8') as f:
                json.dump({"status": "starting", "message": "ä»»åŠ¡æ­£åœ¨åˆå§‹åŒ–..."}, f)

        except Exception as e:
            print(f"åˆå§‹åŒ–çŠ¶æ€æ–‡ä»¶æ—¶å‡ºé”™: {e}")

        python_executable = sys.executable or "python"
        command_args = " ".join(urls_to_crawl)
        command_str = (
            f'"{python_executable}" "{CRAWLER_SCRIPT_PATH}" {command_args} '
            f'> "{CRAWLER_LOG_FILE}" 2>&1'
        )

        print(f"--- [Admin Task] å¼‚æ­¥æ‰§è¡Œçˆ¬è™« (åŒæ­¥å¯åŠ¨æ£€æŸ¥) ---")
        print(f"CMD (Shell Mode): {command_str}")

        process = subprocess.Popen(
            command_str,
            shell=True,
            encoding='utf-8',
            errors='replace'
        )

        timeout = 5
        start_time = time.time()
        while time.time() - start_time < timeout:
            time.sleep(0.5)

            if os.path.exists(CRAWLER_STATUS_FILE):
                try:
                    with open(CRAWLER_STATUS_FILE, 'r', encoding='utf-8') as f:
                        status_data = json.load(f)
                        if status_data.get('status') in ['running', 'error']:
                            print(f"   -> (å¯åŠ¨æ£€æŸ¥) çŠ¶æ€å·²æ›´æ–°ä¸º: {status_data.get('status')}")

                            return jsonify({"message": f"æœåŠ¡å™¨å·²æ¥å—ä»»åŠ¡ï¼Œå¯åŠ¨æ£€æŸ¥é€šè¿‡ã€‚"}), 200
                except Exception:
                    pass

        if process.poll() is None:
            if os.name == 'nt':
                subprocess.run(['taskkill', '/F', '/T', '/PID', str(process.pid)], capture_output=True, text=True)
            else:
                subprocess.run(['kill', '-9', str(process.pid)], capture_output=True, text=True)

            with open(CRAWLER_STATUS_FILE, 'w', encoding='utf-8') as f:
                json.dump({"status": "error", "message": "å¯åŠ¨è¶…æ—¶ (5ç§’)ï¼Œå­è¿›ç¨‹å·²å¼ºåˆ¶ç»ˆæ­¢ã€‚"}, f)

            return jsonify({"error": "å¯åŠ¨è¶…æ—¶ï¼šå­è¿›ç¨‹æœªåœ¨ 5 ç§’å†…æŠ¥å‘ŠçŠ¶æ€ã€‚", "details": "è¯·æŸ¥çœ‹ crawler.log æ–‡ä»¶"}), 500
        else:
            with open(CRAWLER_STATUS_FILE, 'w', encoding='utf-8') as f:
                json.dump({"status": "error", "message": "å­è¿›ç¨‹å¯åŠ¨åç«‹åˆ»å´©æºƒï¼Œé€€å‡ºä»£ç éé›¶ã€‚"}, f)
            return jsonify({"error": "å¯åŠ¨å¤±è´¥ï¼šå­è¿›ç¨‹ç«‹åˆ»å´©æºƒã€‚", "details": "è¯·æ£€æŸ¥ crawler.log æ–‡ä»¶"}), 500

    except Exception as e:
        traceback.print_exc()
        with open(CRAWLER_STATUS_FILE, 'w', encoding='utf-8') as f:
            json.dump({"status": "error", "message": "å¯åŠ¨è¿›ç¨‹å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç»ˆç«¯æ—¥å¿—ã€‚"}, f)
        return jsonify({"error": f"å¯åŠ¨çˆ¬è™«æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {str(e)}"}), 500


@app.route('/api/admin/system/crawler_logs', methods=['GET'])
@admin_required()
def admin_get_crawler_logs():
    try:
        if not os.path.exists(CRAWLER_LOG_FILE):
            return jsonify({"logs": "ä»»åŠ¡æœªå¯åŠ¨æˆ–æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨ã€‚", "status": "idle"})

        # è¯»å–æ—¥å¿—æ–‡ä»¶å†…å®¹
        with open(CRAWLER_LOG_FILE, 'r', encoding='utf-8') as f:
            logs = f.read()

        # è¯»å–çŠ¶æ€æ–‡ä»¶ä»¥ç¡®å®šä»»åŠ¡æ˜¯å¦ä»åœ¨è¿è¡Œ
        current_status = "running"
        if os.path.exists(CRAWLER_STATUS_FILE):
            try:
                with open(CRAWLER_STATUS_FILE, 'r', encoding='utf-8') as f:
                    status_data = json.load(f)
                    current_status = status_data.get('status', 'running')
            except Exception:
                pass

        return jsonify({"logs": logs, "status": current_status}), 200

    except Exception as e:
        traceback.print_exc()
        return jsonify({"logs": f"æ— æ³•è¯»å–æ—¥å¿—æ–‡ä»¶: {str(e)}", "status": "error"}), 500


@app.route('/api/admin/system/stop_crawler', methods=['POST'])
@admin_required()  #
def admin_stop_crawler():
    try:
        if not os.path.exists(CRAWLER_STATUS_FILE):
            return jsonify({"error": "æ²¡æœ‰æ‰¾åˆ°çŠ¶æ€æ–‡ä»¶ï¼Œæ— æ³•åœæ­¢ã€‚"}), 404

        pid_to_kill = None
        try:
            with open(CRAWLER_STATUS_FILE, 'r', encoding='utf-8') as f:
                status_data = json.load(f)
                pid_to_kill = status_data.get('pid')
        except Exception as e:
            print(f"è¯»å–çŠ¶æ€æ–‡ä»¶æ—¶å‡ºé”™: {e}ã€‚å°†å¼ºåˆ¶é‡ç½®ã€‚")

        try:
            if not pid_to_kill:
                print(f"--- [Admin Task] çŠ¶æ€æ–‡ä»¶ä¸­æ²¡æœ‰PIDï¼Œä»…æ¸…ç†çŠ¶æ€ã€‚ ---")
            else:
                print(f"--- [Admin Task] æ­£åœ¨å°è¯•åœæ­¢ PID: {pid_to_kill} ---")

                if os.name == 'nt':
                    command = ['taskkill', '/F', '/T', '/PID', str(pid_to_kill)]
                else:
                    command = ['kill', '-9', str(pid_to_kill)]

                stdout, stderr = _run_subprocess(command)

                if stderr:
                    print(f"--- [Admin Task] åœæ­¢å‘½ä»¤è¿”å›ä¿¡æ¯ (å¯èƒ½æ˜¯è¿›ç¨‹å·²ä¸å­˜åœ¨): {stderr} ---")

        except Exception as e:
            print(f"åœæ­¢è¿›ç¨‹ {pid_to_kill} æ—¶å‡ºé”™ (å¯èƒ½æ˜¯è¿›ç¨‹å·²ä¸å­˜åœ¨): {e}")

        finally:
            with open(CRAWLER_STATUS_FILE, 'w', encoding='utf-8') as f:
                json.dump(
                    {"status": "idle", "message": f"ä»»åŠ¡ (PID: {pid_to_kill or 'æœªçŸ¥'}) å·²è¢«ç®¡ç†å‘˜å¼ºåˆ¶åœæ­¢æˆ–æ¸…ç†ã€‚"}, f)

            print(f"--- [Admin Task] çŠ¶æ€æ–‡ä»¶å·²å¼ºåˆ¶é‡ç½®ä¸º idle ---")

        return jsonify({"message": f"æˆåŠŸå‘é€åœæ­¢å‘½ä»¤å¹¶æ¸…ç†çŠ¶æ€ (PID: {pid_to_kill})ã€‚"})

    except Exception as e:
        traceback.print_exc()
        return jsonify({"error": f"åœæ­¢ä»»åŠ¡æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {str(e)}"}), 500


@app.route('/api/admin/system/crawler_status', methods=['GET'])
@admin_required()  #
def admin_get_crawler_status():
    try:
        if not os.path.exists(CRAWLER_STATUS_FILE):
            return jsonify({"status": "idle", "message": "çˆ¬è™«å¤„äºç©ºé—²çŠ¶æ€ã€‚"})

        with open(CRAWLER_STATUS_FILE, 'r', encoding='utf-8') as f:
            status_data = json.load(f)
            return jsonify(status_data)

    except json.JSONDecodeError:
        return jsonify({"status": "error", "message": "é”™è¯¯ï¼šçŠ¶æ€æ–‡ä»¶ (crawler.status) æ ¼å¼æŸåã€‚"})
    except Exception as e:
        return jsonify({"status": "error", "message": f"è¯»å–çŠ¶æ€æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {str(e)}"}), 500


@app.route('/api/admin/system/download_latest_csv', methods=['GET'])
@admin_required()
def admin_download_latest_csv():
    try:
        script_dir = os.path.dirname(__file__)
        file_path = os.path.join(script_dir, CRAWLER_OUTPUT_FILE)

        if not os.path.exists(file_path):
            return jsonify({"error": f"æ–‡ä»¶ '{CRAWLER_OUTPUT_FILE}' æœªæ‰¾åˆ°ã€‚è¯·å…ˆæ‰§è¡Œçˆ¬è™«ã€‚"}), 404

        dynamic_filename = CRAWLER_OUTPUT_FILE
        if os.path.exists(CRAWLER_STATUS_FILE):
            try:
                with open(CRAWLER_STATUS_FILE, 'r', encoding='utf-8') as f:
                    status_data = json.load(f)
                    message = status_data.get('message', '')
                    match = re.search(r"æ•°æ®å·²ä¿å­˜ä¸º:\s*(.+)", message)
                    if match:
                        dynamic_filename = match.group(1).strip()
            except Exception:
                pass

        return send_from_directory(
            script_dir,
            CRAWLER_OUTPUT_FILE,
            as_attachment=True,
            mimetype='text/csv',
            download_name=dynamic_filename
        )
    except Exception as e:
        traceback.print_exc()
        return jsonify({"error": f"ä¸‹è½½æ–‡ä»¶æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {str(e)}"}), 500


@app.route('/api/admin/system/import_latest_csv', methods=['POST'])
@admin_required()  #
def admin_import_latest_csv():
    try:
        python_executable = sys.executable or "python"
        csv_path = os.path.join(os.path.dirname(__file__), CRAWLER_OUTPUT_FILE)

        if not os.path.exists(csv_path):
            return jsonify({"error": f"æ–‡ä»¶ '{CRAWLER_OUTPUT_FILE}' æœªæ‰¾åˆ°ã€‚è¯·å…ˆæ‰§è¡Œçˆ¬è™«ã€‚"}), 404

        print(f"--- [Admin Task] åŒæ­¥å¯¼å…¥æœ€æ–°çˆ¬è™«æ•°æ® ---")
        command = [python_executable, IMPORTER_SCRIPT_PATH, csv_path]
        print(f"CMD: {' '.join(command)}")

        stdout, stderr = _run_subprocess(command)

        if stderr:
            return jsonify({"error": stderr}), 500

        return jsonify({"message": f"å¯¼å…¥æœåŠ¡å™¨æ–‡ä»¶ '{CRAWLER_OUTPUT_FILE}' æˆåŠŸï¼š\n\n{stdout}"}), 200

    except Exception as e:
        traceback.print_exc()
        return jsonify({"error": f"å¯¼å…¥æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {str(e)}"}), 500


@app.route('/api/admin/system/upload_and_import', methods=['POST'])
@admin_required()  #
def admin_upload_and_import():
    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡ä»¶
        if 'files[]' not in request.files:
            return jsonify({"error": "æœªæ‰¾åˆ° 'files[]' æ–‡ä»¶éƒ¨åˆ†"}), 400

        files = request.files.getlist('files[]')

        if not files or all(f.filename == '' for f in files):
            return jsonify({"error": "æ²¡æœ‰é€‰æ‹©ä»»ä½•æ–‡ä»¶"}), 400

        temp_file_paths = []
        imported_filenames = []

        for file in files:
            if file and file.filename.endswith('.csv'):
                filename = secure_filename(file.filename)
                temp_path = os.path.join(UPLOAD_FOLDER, filename)
                file.save(temp_path)  #
                temp_file_paths.append(temp_path)
                imported_filenames.append(filename)

        if not temp_file_paths:
            return jsonify({"error": "æ²¡æœ‰ä¸Šä¼ æœ‰æ•ˆçš„ .csv æ–‡ä»¶"}), 400

        python_executable = sys.executable or "python"
        command = [python_executable, IMPORTER_SCRIPT_PATH] + temp_file_paths

        print(f"--- [Admin Task] åŒæ­¥å¯¼å…¥ä¸Šä¼ çš„ {len(temp_file_paths)} ä¸ªæ–‡ä»¶ ---")
        print(f"CMD: {' '.join(command)}")

        stdout, stderr = _run_subprocess(command)

        print(f"--- [Admin Task] æ¸…ç† {len(temp_file_paths)} ä¸ªä¸´æ—¶æ–‡ä»¶ ---")
        for path in temp_file_paths:
            try:
                os.remove(path)
            except Exception as e:
                print(f"è­¦å‘Šï¼šåˆ é™¤ä¸´æ—¶æ–‡ä»¶ {path} å¤±è´¥: {e}")

        if stderr:
            return jsonify({"error": f"å¯¼å…¥æ–‡ä»¶æ—¶å‡ºé”™ï¼š\n\n{stderr}"}), 500

        return jsonify({
            "message": f"æˆåŠŸå¤„ç† {len(imported_filenames)} ä¸ªä¸Šä¼ æ–‡ä»¶ï¼š\n{', '.join(imported_filenames)}\n\nã€å¯¼å…¥æ—¥å¿—ã€‘:\n{stdout}"}), 200

    except Exception as e:
        traceback.print_exc()
        return jsonify({"error": f"ä¸Šä¼ æ–‡ä»¶æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {str(e)}"}), 500


@app.route('/debug/env')
def debug_env():
    """ä¸€ä¸ªä¸´æ—¶çš„è°ƒè¯•æ¥å£ï¼Œç”¨æ¥æ˜¾ç¤º Flask æ­£åœ¨ä½¿ç”¨çš„ Python.exe è·¯å¾„"""
    print(f"Flask is using this Python: {sys.executable}")
    return f"Flask (app_ds2.py) æ­£åœ¨ä½¿ç”¨çš„ Python.exe è·¯å¾„æ˜¯: <br><br><strong>{sys.executable}</strong><br><br>è¯·å¤åˆ¶è¿™æ¡å®Œæ•´è·¯å¾„ã€‚"


if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5001, debug=True)
